{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017921,
     "end_time": "2023-12-12T12:44:48.731519",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.713598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/cover_image.png?raw=true\" width=\"2400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 目标检测概述</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"><strong>目标检测</strong> 是一种识别图像或视频流中物体位置和类别的技术。它的输出包括每个物体的边界框（用于包围物体）、类别标签以及置信度分数。当不需要精确的物体形状细节时，这种方法非常适合快速定位场景中的物体。目前，用于此任务的主要模型包括 <strong>RCNN</strong>、<strong>SSD</strong> 和 <strong>YOLO</strong>。其中，YOLO 模型表现尤为出色，尤其在实时检测场景中，它在速度和精度的平衡上优于其他模型。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚀 YOLO：实时目标检测革新方法</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <strong>YOLO</strong>（即 “<strong>You Only Look Once</strong>” 的缩写）通过提供一种快速且高效的方法，彻底改变了计算机视觉中的目标检测方式。它将目标定位和分类结合为一步操作，极大地简化了检测流程。这种创新方法使YOLO能够以高精度实时处理图像和视频。最新版本的<strong>YOLOv8</strong>进一步提升了这项技术，能够出色地应对目标检测和图像分割中的挑战。YOLO的实时能力使其成为需要快速精准识别目标的应用场景中的首选。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🌟 YOLOv8：目标检测技术的前沿突破</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <strong>YOLOv8</strong> 由 <strong>Ultralytics</strong> 在 <strong>2023年1月</strong> 推出，是YOLO系列人工智能模型的最新成果。YOLOv8适用于分类、目标检测和图像分割等任务，其精度和速度都优于前代产品YOLOv7。该模型以<strong>Darknet53</strong>为骨干网络，使用更多的特征图和高效的卷积神经网络，从而实现了更高的平均精度（mAP）和每秒帧数（fps）。YOLOv8引入了一种创新的无锚点检测头，能够像图像分割技术一样进行像素级边界框估计，并采用了新的损失函数。这些特性的结合使YOLOv8在<strong>COCO基准数据集</strong>上达到了<strong>53.7%</strong>的平均精度（<strong>mean Average Precision</strong>），使其成为目标检测领域的顶尖模型。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚦 交通流量估计项目概述</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        借助YOLO的实时检测能力，本项目专注于<strong>交通流量估计</strong>，这是城市和交通管理中的一个重要组成部分。项目的目标是在每一帧图像中统计特定区域内的车辆数量，以评估交通流量。这些有价值的数据有助于识别交通高峰时段、拥堵区域，并协助城市规划。通过本项目，我们旨在开发一套全面的工具，提供关于交通流动和模式的详细见解，以增强交通管理和城市规划策略。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🎯 项目目标</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>YOLOv8模型选择与初步评估：</strong> 从YOLOv8的预训练模型开始，评估其在COCO数据集上对车辆检测的初始性能。</li>\n",
    "        <li><strong>专业车辆数据集准备：</strong> 整理和标注专门的车辆数据集，以优化模型对多种车辆类型的检测能力。</li>\n",
    "        <li><strong>模型微调以提升车辆检测性能：</strong> 使用迁移学习对YOLOv8模型进行微调，专注于从空中视角检测车辆，以提高检测的精确度和召回率。</li>\n",
    "        <li><strong>全面评估模型性能：</strong> 分析学习曲线，评估混淆矩阵，并评估性能指标，以验证模型的准确性和泛化能力。</li>\n",
    "        <li><strong>在测试数据上进行推理和泛化：</strong> 在验证图像、未见过的测试图像和测试视频上测试模型的泛化能力，以展示其实用性和有效性。</li>\n",
    "        <li><strong>实时交通流量估计：</strong> 实现一种算法，通过在测试视频数据上实时统计车辆数量和分析交通强度，来估计交通流量。</li>\n",
    "        <li><strong>跨平台模型部署准备：</strong> 将微调后的模型导出为ONNX格式，以便在不同平台和环境中灵活使用。</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016739,
     "end_time": "2023-12-12T12:44:48.938242",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.921503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"contents_tabel\"></a>   \n",
    "\n",
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📋 Table of Contents</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><a href=\"#Initialization\" style=\"text-decor\n",
    "            ation: none;\">Step 1 | Setup and Initialization</a></li>\n",
    "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">Step 2 | Loading YOLOv8 Pre-trained Model</a></li>\n",
    "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">Step 3 | Dataset Exploration</a></li>\n",
    "        <li><a href=\"#Fine_Tuning_YOLOv8\" style=\"text-decoration: none;\">Step 4 | Fine-Tuning YOLOv8</a></li>\n",
    "        <li><a href=\"#Model_Performance_Evaluation\" style=\"text-decoration: none;\">Step 5 | Model Performance Evaluation</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">Step 5.1 | Learning Curves Analysis</a></li>\n",
    "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">Step 5.2 | Confusion Matrix Evaluation</a></li>\n",
    "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">Step 5.3 | Performance Metrics Assessment</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">Step 6 | Model Inference & Generalization Assessment</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Inference_Validation\" style=\"text-decoration: none;\">Step 6.1 | Inference on Validation Set Images</a></li>\n",
    "                <li><a href=\"#Inference_Test_Image\" style=\"text-decoration: none;\">Step 6.2 | Inference on an Unseen Test Image</a></li>\n",
    "                <li><a href=\"#Inference_Test_Video\" style=\"text-decoration: none;\">Step 6.3 | Inference on an Unseen Test Video</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Traffic_Intensity_Estimation\" style=\"text-decoration: none;\">Step 7 | Real-Time Traffic Intensity Estimation</a></li>\n",
    "        <li><a href=\"#Model_Export\" style=\"text-decoration: none;\">Step 8 | Model Export for Cross-Platform Deployment</a></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents_table\"></a>   \n",
    "\n",
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📋 目录</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><a href=\"#Initialization\" style=\"text-decoration: none;\">步骤 1 | 环境设置与初始化</a></li>\n",
    "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">步骤 2 | 加载 YOLOv8 预训练模型</a></li>\n",
    "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">步骤 3 | 数据集探索</a></li>\n",
    "        <li><a href=\"#Fine_Tuning_YOLOv8\" style=\"text-decoration: none;\">步骤 4 | YOLOv8 微调</a></li>\n",
    "        <li><a href=\"#Model_Performance_Evaluation\" style=\"text-decoration: none;\">步骤 5 | 模型性能评估</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">步骤 5.1 | 学习曲线分析</a></li>\n",
    "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">步骤 5.2 | 混淆矩阵评估</a></li>\n",
    "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">步骤 5.3 | 性能指标评估</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">步骤 6 | 模型推理与泛化能力评估</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Inference_Validation\" style=\"text-decoration: none;\">步骤 6.1 | 验证集图像推理</a></li>\n",
    "                <li><a href=\"#Inference_Test_Image\" style=\"text-decoration: none;\">步骤 6.2 | 未见测试图像推理</a></li>\n",
    "                <li><a href=\"#Inference_Test_Video\" style=\"text-decoration: none;\">步骤 6.3 | 未见测试视频推理</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Traffic_Intensity_Estimation\" style=\"text-decoration: none;\">步骤 7 | 实时交通流量估计</a></li>\n",
    "        <li><a href=\"#Model_Export\" style=\"text-decoration: none;\">步骤 8 | 模型导出以实现跨平台部署</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017755,
     "end_time": "2023-12-12T12:44:48.973007",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.955252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color=#141140>Let's get started:</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016921,
     "end_time": "2023-12-12T12:44:49.006952",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.990031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Initialization\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤一 | 环境设置与初始化</p>\n",
    "⬆️ [返回主目录](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017387,
     "end_time": "2023-12-12T12:44:49.08451",
     "exception": false,
     "start_time": "2023-12-12T12:44:49.067123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">YOLOv8 is the first version of the YOLO series to offer an official package that can be easily installed using pip. This streamlines the setup process, a notable advancement from previous versions that required cloning repositories and running scripts. Let's begin by installing the Ultralytics package:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">YOLOv8是YOLO系列中首个提供官方软件包的版本，可以通过pip轻松安装。这大大简化了设置过程，与之前需要克隆仓库并运行脚本的版本相比，这是一个显著的进步。我们先从安装Ultralytics软件包开始：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:14.811943Z",
     "iopub.status.busy": "2024-01-07T16:41:14.81127Z",
     "iopub.status.idle": "2024-01-07T16:41:28.603129Z",
     "shell.execute_reply": "2024-01-07T16:41:28.602158Z",
     "shell.execute_reply.started": "2024-01-07T16:41:14.811904Z"
    },
    "papermill": {
     "duration": 13.887399,
     "end_time": "2023-12-12T12:45:02.989658",
     "exception": false,
     "start_time": "2023-12-12T12:44:49.102259",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install Ultralytics library\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018229,
     "end_time": "2023-12-12T12:45:03.027025",
     "exception": false,
     "start_time": "2023-12-12T12:45:03.008796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">接下来，导入项目所需的全部重要库：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:28.605394Z",
     "iopub.status.busy": "2024-01-07T16:41:28.605102Z",
     "iopub.status.idle": "2024-01-07T16:41:33.110352Z",
     "shell.execute_reply": "2024-01-07T16:41:33.109382Z",
     "shell.execute_reply.started": "2024-01-07T16:41:28.605368Z"
    },
    "papermill": {
     "duration": 5.616125,
     "end_time": "2023-12-12T12:45:08.66149",
     "exception": false,
     "start_time": "2023-12-12T12:45:03.045365",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Disable warnings in the notebook to maintain clean output cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.111939Z",
     "iopub.status.busy": "2024-01-07T16:41:33.111517Z",
     "iopub.status.idle": "2024-01-07T16:41:33.116783Z",
     "shell.execute_reply": "2024-01-07T16:41:33.115938Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.111912Z"
    },
    "papermill": {
     "duration": 0.027946,
     "end_time": "2023-12-12T12:45:08.709398",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.681452",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure the visual appearance of Seaborn plots\n",
    "sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019009,
     "end_time": "2023-12-12T12:45:08.747327",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.728318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Load_Model\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤 2 | 加载 YOLOv8 预训练模型</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018944,
     "end_time": "2023-12-12T12:45:08.785891",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.766947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Here are the pre-trained YOLOv8 object detection models, which have been trained on the <strong>COCO dataset</strong>. The Common Objects in Context (COCO) dataset is extensive, designed for object detection, segmentation, and captioning, and encompasses <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80 diverse object categories</a>:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">这里提供了在 <strong>COCO数据集</strong> 上预训练的YOLOv8目标检测模型。COCO（Common Objects in Context）数据集内容丰富，专为物体检测、分割和标注设计，涵盖了 <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80种不同的物体类别</a>。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018452,
     "end_time": "2023-12-12T12:45:08.823299",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.804847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/YOLOv8_object_detection_models.jpg?raw=true\" width=\"2400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018195,
     "end_time": "2023-12-12T12:45:08.860376",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.842181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📈 Model Performance Trade-offs</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The YOLOv8 suite presents five distinct models: <strong>nano</strong>, <strong>small</strong>, <strong>medium</strong>, <strong>large</strong>, and <strong>xlarge</strong>. A clear trend emerges from the data: as model size increases, there's a notable improvement in <strong>mAP</strong>, indicating enhanced accuracy. Conversely, this augmentation comes at the cost of speed, with larger models being slower. All models adhere to a standard input size of <strong>640x640</strong> pixels, optimizing performance across diverse applications.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📈 模型性能权衡</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        YOLOv8系列提供了五种不同型号的模型：<strong>微型</strong>（nano）、<strong>小型</strong>（small）、<strong>中型</strong>（medium）、<strong>大型</strong>（large）和<strong>超大型</strong>（xlarge）。数据显示出一个明显的趋势：随着模型尺寸的增大，<strong>mAP</strong>（平均精度）有显著提升，这意味着更高的准确性。然而，这种提升是以速度为代价的，因为更大的模型运行速度更慢。所有模型都采用统一的输入尺寸<strong>640x640</strong>像素，以优化在不同应用中的性能。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018694,
     "end_time": "2023-12-12T12:45:08.897562",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.878868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Intersection Over Union (IoU)</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        IoU is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between the predicted bounding box and the ground truth, with values ranging from 0 (no overlap) to 1 (perfect overlap). IoU is crucial for determining whether a detection is a true positive or a false positive, often using a threshold like 0.5 or 0.75 to make this distinction.\n",
    "    </p>\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Mean Average Precision (mAP)</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        mAP is a commonly used metric to evaluate the precision of object detection models. It is the average of the AP (Average Precision) calculated for all the classes and is based on the area under the precision-recall curve. This metric reflects the model's precision across different levels of recall, providing a comprehensive performance measure that accounts for both the detection accuracy and the ability to detect all relevant objects.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 交并比（IoU）</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        交并比（IoU）是用于评估目标检测器在特定数据集上准确性的指标。它衡量预测边界框与真实边界框之间的重叠程度，取值范围从0（无重叠）到1（完全重叠）。IoU对于判断检测结果是真正例还是假正例至关重要，通常使用0.5或0.75这样的阈值来进行区分。\n",
    "    </p>\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 平均精度均值（mAP）</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        平均精度均值（mAP）是评估目标检测模型精度的常用指标。它是所有类别平均精度（AP）的平均值，基于精确率-召回率曲线下的面积计算。这一指标反映了模型在不同召回率水平下的精确率，提供了一个综合衡量模型性能的指标，既考虑了检测的准确性，也考虑了检测所有相关目标的能力。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018603,
     "end_time": "2023-12-12T12:45:08.935075",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.916472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">For our real-time traffic density estimation application, I am going to select the <strong>YOLOv8 nano pre-trained model (yolov8n.pt)</strong> to handle vehicle detection. This model ensures the fastest possible inference time, making it well-suited for real-time use:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">在本项目实时交通流量估计应用中，将选择<strong>YOLOv8微型预训练模型（yolov8n.pt）</strong>来处理车辆检测。该模型确保了最快的推理时间，非常适合实时使用。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.118523Z",
     "iopub.status.busy": "2024-01-07T16:41:33.118193Z",
     "iopub.status.idle": "2024-01-07T16:41:34.004717Z",
     "shell.execute_reply": "2024-01-07T16:41:34.00379Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.118492Z"
    },
    "papermill": {
     "duration": 0.536317,
     "end_time": "2023-12-12T12:45:09.490231",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.953914",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model from Ultralytics\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018601,
     "end_time": "2023-12-12T12:45:09.528765",
     "exception": false,
     "start_time": "2023-12-12T12:45:09.510164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">The pre-trained model we've loaded is trained on the COCO dataset, which includes the 'car' and 'truck' classes among its 80 different categories — exactly what we need for our project. Now, let's put our model to the test and see how it performs on a sample image:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">上面加载的预训练模型是在COCO数据集上训练的，其中包含80个不同类别中的“汽车”和“卡车”类别——这正是我们项目所需要的。下面将对模型进行测试，看看它在样本图像上的表现如何：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:34.008052Z",
     "iopub.status.busy": "2024-01-07T16:41:34.007757Z",
     "iopub.status.idle": "2024-01-07T16:41:36.773138Z",
     "shell.execute_reply": "2024-01-07T16:41:36.771864Z",
     "shell.execute_reply.started": "2024-01-07T16:41:34.008028Z"
    },
    "papermill": {
     "duration": 8.967725,
     "end_time": "2023-12-12T12:45:18.515569",
     "exception": false,
     "start_time": "2023-12-12T12:45:09.547844",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
    "\n",
    "# Perform inference on the provided image(s)\n",
    "results = model.predict(source=image_path, \n",
    "                        imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n",
    "                        conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n",
    "\n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04642,
     "end_time": "2023-12-12T12:45:18.611612",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.565192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Pre-trained Model Detection Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        In our sample image, the pre-trained model missed the detectable truck and car that were clearly visible. A model pre-trained on a dataset with a broad range of classes, like COCO's 80 different categories, may not perform as well on a specific subset of those categories due to the diversity of objects it has been trained to recognize. If we fine-tune this model on a specialized dataset that focuses solely on vehicles, it can learn to detect various types of vehicles more accurately. Fine-tuning on a vehicle-specific dataset allows the model to become more specialized, adjusting the weights to be more sensitive to features specific to vehicles. As a result, the model's mean Average Precision (mAP) for vehicle detection could improve because it's being optimized on a narrower, more relevant range of classes for our specific application. Fine-tuning also helps the model generalize better for vehicle detection tasks, potentially reducing false negatives (like missing a detectable truck) and improving overall detection performance.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 预训练模型检测分析</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        在我们的样本图像中，预训练模型遗漏了明显可见的卡车和汽车。在像COCO这样的包含80个不同类别的广泛数据集上预训练的模型，由于其训练识别的物体种类繁多，在特定子类别上的表现可能不如在专门针对车辆的数据集上训练的模型[^14^]。如果我们在这个模型上针对专门的车辆数据集进行微调，它可以更准确地检测各种类型的车辆。针对车辆的专门数据集进行微调可以使模型变得更加专业化，调整权重以对车辆的特定特征更加敏感[^14^]。因此，模型在车辆检测上的平均精度均值（mAP）可能会提高，因为它正在针对我们特定应用的相关性更窄的类别范围进行优化[^15^]。微调还有助于模型更好地泛化车辆检测任务，可能会减少假阴性（如遗漏可检测的卡车）并提高整体检测性能[^14^]。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046519,
     "end_time": "2023-12-12T12:45:18.70644",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.659921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Dataset_Exploration\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Dataset Exploration</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04706,
     "end_time": "2023-12-12T12:45:18.800921",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.753861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Dataset Preparation for Model Fine-tuning</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To fine-tune our pre-trained model on a specialized dataset that focuses solely on vehicles, so that it can learn to detect various types of vehicles more accurately, I have prepared a dataset which is available at this link <a href=\"https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset\">Top-View Vehicle Detection Image Dataset for YOLOv8</a>. The dataset zeroes in on the 'Vehicle' class, covering a wide variety of vehicles such as cars, trucks, and buses. It is composed of 626 images sourced from top-view perspectives, <strong>annotated meticulously in the YOLOv8 format</strong> for effective vehicle detection.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The dataset undergoes a standardization process where each image is resized to a uniform resolution of <strong>640x640 pixels</strong>. To bolster the model's ability to generalize, augmentations were applied to the training data, which consists of 536 images. The validation set contains 90 images and remains unaugmented to preserve the integrity of performance evaluation.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        This dataset, curated from <a href=\"https://www.pexels.com/search/videos/\">Pexels</a>, captures the diversity of vehicles from an aerial view, making it ideal for highway monitoring tasks. Each video frame was selected at a sampling rate of 1 frame per second using <a href=\"https://universe.roboflow.com/farzad/vehicle_detection_yolov8\">Roboflow</a>, which facilitated precise annotation for object detection.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046033,
     "end_time": "2023-12-12T12:45:18.893777",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.847744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ YOLOv8 Dataset Format</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Our dataset, structured for YOLOv8 format, has been meticulously prepared on Roboflow. It encompasses all necessary components for an efficient object detection model training. Here’s a detailed breakdown:\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>1️⃣ Train Directory:</b><br>\n",
    "        The 'train' directory houses our training dataset. It includes 536 images within the 'images' subfolder and corresponding YOLOv8 format labels in the 'labels' subfolder.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>2️⃣ Validation Directory:</b><br>\n",
    "        The 'valid' directory contains the validation dataset. This consists of 90 images in the 'images' subfolder and their respective YOLOv8 format labels in the 'labels' subfolder.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>3️⃣ data.yaml:</b><br>\n",
    "        This file is the Ultralytics YOLO dataset configuration file. It specifies paths to the training and validation datasets, defines the number of classes (1), and the class name ('Vehicle'). This format is crucial for setting up and training the model accurately with our dataset.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>📝 Note about labels:</b><br>\n",
    "        Labels in our dataset are formatted in YOLO style, where each image is associated with a *.txt file. These files describe the detected objects in a '<strong>class x_center y_center width height</strong>' format. Importantly, the box coordinates are normalized between 0 and 1. If an image has no detectable objects, it won’t have a corresponding *.txt file.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046651,
     "end_time": "2023-12-12T12:45:18.986753",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.940102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's begin our exploration by examining the '<strong>data.yaml</strong>' file:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.774876Z",
     "iopub.status.busy": "2024-01-07T16:41:36.774521Z",
     "iopub.status.idle": "2024-01-07T16:41:36.788027Z",
     "shell.execute_reply": "2024-01-07T16:41:36.787189Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.774845Z"
    },
    "papermill": {
     "duration": 0.066253,
     "end_time": "2023-12-12T12:45:19.099731",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.033478",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the dataset_path\n",
    "dataset_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset'\n",
    "\n",
    "# Set the path to the YAML file\n",
    "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
    "\n",
    "# Load and print the contents of the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(yaml.dump(yaml_content, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046159,
     "end_time": "2023-12-12T12:45:19.194321",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.148162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ Understanding the data.yaml File</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        As I previously mentioned, the 'data.yaml' file is a key part of setting up our model. It points out where to find the training and validation images and tells the model that we're focusing on just one class, named 'Vehicle'. This file is essential for making sure our Ultralytics YOLOv8 model learns from our specific dataset, as it guides the model to understand exactly what it needs to look for and where.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046625,
     "end_time": "2023-12-12T12:45:19.287516",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.240891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Now, let's continue our exploration by counting the images in both the training and validation sets and verifying their sizes:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.789658Z",
     "iopub.status.busy": "2024-01-07T16:41:36.789244Z",
     "iopub.status.idle": "2024-01-07T16:41:39.11087Z",
     "shell.execute_reply": "2024-01-07T16:41:39.109926Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.789629Z"
    },
    "papermill": {
     "duration": 4.275063,
     "end_time": "2023-12-12T12:45:23.611095",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.336032",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set paths for training and validation image sets\n",
    "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# Initialize counters for the number of images\n",
    "num_train_images = 0\n",
    "num_valid_images = 0\n",
    "\n",
    "# Initialize sets to hold the unique sizes of images\n",
    "train_image_sizes = set()\n",
    "valid_image_sizes = set()\n",
    "\n",
    "# Check train images sizes and count\n",
    "for filename in os.listdir(train_images_path):\n",
    "    if filename.endswith('.jpg'):  \n",
    "        num_train_images += 1\n",
    "        image_path = os.path.join(train_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            train_image_sizes.add(img.size)\n",
    "\n",
    "# Check validation images sizes and count\n",
    "for filename in os.listdir(valid_images_path):\n",
    "    if filename.endswith('.jpg'): \n",
    "        num_valid_images += 1\n",
    "        image_path = os.path.join(valid_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            valid_image_sizes.add(img.size)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of validation images: {num_valid_images}\")\n",
    "\n",
    "# Check if all images in training set have the same size\n",
    "if len(train_image_sizes) == 1:\n",
    "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Training images have varying sizes.\")\n",
    "\n",
    "# Check if all images in validation set have the same size\n",
    "if len(valid_image_sizes) == 1:\n",
    "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Validation images have varying sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051006,
     "end_time": "2023-12-12T12:45:23.712196",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.66119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📊 Dataset Analysis Insights</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The dataset for our project consists of 536 training images and 90 validation images, all uniformly sized at 640x640 pixels. This size aligns with the benchmark standard for the YOLOv8 model, ensuring optimal accuracy and speed during model performance. The split ratio of approximately 85% for training and 15% for validation provides a substantial amount of data for model learning while retaining enough images for effective model validation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047477,
     "end_time": "2023-12-12T12:45:23.809043",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.761566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's take a look at a few images from the training dataset to get a sense of what the data looks like:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:39.112348Z",
     "iopub.status.busy": "2024-01-07T16:41:39.112057Z",
     "iopub.status.idle": "2024-01-07T16:41:41.729949Z",
     "shell.execute_reply": "2024-01-07T16:41:41.728821Z",
     "shell.execute_reply.started": "2024-01-07T16:41:39.112322Z"
    },
    "papermill": {
     "duration": 2.758546,
     "end_time": "2023-12-12T12:45:26.614041",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.855495",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 8 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n",
    "\n",
    "# Create a 2x4 subplot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 11))\n",
    "\n",
    "# Display each of the selected images\n",
    "for ax, img_file in zip(axes.ravel(), selected_images):\n",
    "    img_path = os.path.join(train_images_path, img_file)\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.suptitle('Sample Images from Training Dataset', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095762,
     "end_time": "2023-12-12T12:45:26.808073",
     "exception": false,
     "start_time": "2023-12-12T12:45:26.712311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Fine_Tuning_YOLOv8\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Fine-Tuning YOLOv8 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0987,
     "end_time": "2023-12-12T12:45:27.003546",
     "exception": false,
     "start_time": "2023-12-12T12:45:26.904846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step of our project, we're set to fine-tune our YOLOv8 pre-trained object detection model using transfer learning, specifically tailoring it to our 'Top-View Vehicle Detection Image Dataset'. By leveraging the YOLOv8 model's existing weights from its training on the comprehensive COCO dataset, we start from a robust foundation rather than from scratch. This approach not only saves significant time and resources but also capitalizes on our focused dataset to enhance the model's ability to accurately recognize and detect vehicles in top-view images. This method of training enables efficient and effective model adaptation, ensuring it's finely attuned to the specificities of vehicle detection from aerial perspectives:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:41.731615Z",
     "iopub.status.busy": "2024-01-07T16:41:41.731294Z",
     "iopub.status.idle": "2024-01-07T16:55:44.327808Z",
     "shell.execute_reply": "2024-01-07T16:55:44.32653Z",
     "shell.execute_reply.started": "2024-01-07T16:41:41.731585Z"
    },
    "papermill": {
     "duration": 23.627724,
     "end_time": "2023-12-12T12:45:50.728563",
     "exception": true,
     "start_time": "2023-12-12T12:45:27.100839",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model on our custom dataset\n",
    "results = model.train(\n",
    "    data=yaml_file_path,     # Path to the dataset configuration file\n",
    "    epochs=100,              # Number of epochs to train for\n",
    "    imgsz=640,               # Size of input images as integer\n",
    "    device=0,                # Device to run on, i.e. cuda device=0 \n",
    "    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n",
    "    batch=32,                # Number of images per batch\n",
    "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
    "    lr0=0.0001,              # Initial learning rate \n",
    "    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n",
    "    dropout=0.1,             # Use dropout regularization\n",
    "    seed=0                   # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🌐 Integration of Weights & Biases (Wandb) with YOLOv8</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Weights & Biases, known as <strong>wandb.ai</strong>, is an MLOps tool that works seamlessly with Ultralytics, including the YOLOv8 model. When we train our YOLOv8 model, <strong>wandb.ai</strong> helps to manage our machine learning experiments by monitoring the training process, logging important metrics, and saving outputs. It's like a dashboard where we can see how our model is learning, with all the details and visualizations to help us understand the training progress and results.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>🔑 Providing the API Key</b><br>\n",
    "        During the model training, an API key is required for Weights & Biases to track and store our model's data. We'll be prompted to sign up at <a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize</a> to get this key. After signing up, we copy the API key provided and paste it back into our training setup. This key connects our training session to the Weights & Biases platform, allowing us to access all the great features for monitoring and evaluating our model.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Understanding Run Summary Metrics</b></h2>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Learning Rate per Group (lr/pg0, lr/pg1, lr/pg2):</b> These values represent the learning rate for different groups of layers in the neural network. A lower learning rate means the model updates its weights more slowly during training. Consistent learning rates across groups indicate uniform adjustments during the learning process.</li>\n",
    "        <li><b>Mean Average Precision at 50% IoU (metrics/mAP50(B)):</b> This metric measures the model's accuracy in detecting objects with at least 50% Intersection over Union (IoU) with ground truth. A score of 0.97 suggests the model is highly accurate at this IoU threshold.</li>\n",
    "        <li><b>Mean Average Precision across IoU from 50% to 95% (metrics/mAP50-95(B)):</b> This is an average of mAP calculated at different IoU thresholds, from 50% to 95%. A score of 0.74 indicates good overall accuracy across these varying thresholds.</li>\n",
    "        <li><b>Precision (metrics/precision(B)):</b> Precision measures the ratio of correctly predicted positive observations to the total predicted positives. A score of 0.92 means the model is highly precise in its predictions.</li>\n",
    "        <li><b>Recall (metrics/recall(B)):</b> Recall calculates the ratio of correctly predicted positive observations to all observations in actual class. A recall of 0.94 shows the model is very good at finding all relevant cases within the dataset.</li>\n",
    "        <li><b>Model Computational Complexity (model/GFLOPs):</b> Indicates the model's computational demands, with the GFLOPs value suggesting moderate complexity.</li>\n",
    "        <li><b>Model Parameters:</b> This is the total number of trainable parameters in the model. Almost 3 million parameters indicate a model of moderate size and complexity.</li>\n",
    "        <li><b>Inference Speed (model/speed_PyTorch(ms)):</b> The time taken for the model to make a single prediction (inference). 4.6 ms is quite fast, which is good for real-time applications.</li>\n",
    "        <li><b>Training Losses (train/box_loss, train/cls_loss, train/dfl_loss):</b>These are different types of losses during training. 'box_loss' refers to the error in bounding box predictions, 'cls_loss' to classification error, and 'dfl_loss' to distribution focal loss. Lower values indicate better performance.</li>\n",
    "        <li><b>Validation Losses (val/box_loss, val/cls_loss, val/dfl_loss):</b> Similar to training losses, these are losses calculated on the validation dataset. They give an idea of how well the model generalizes to new, unseen data. Almost similar loss values for both training and validation indicate that the model is not overfitting.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Performance_Evaluation\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Model Performance Evaluation </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Post-training, our model generates several files and folders that encapsulate various aspects of the training run. Let's see the list of generated files:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:44.33068Z",
     "iopub.status.busy": "2024-01-07T16:55:44.330087Z",
     "iopub.status.idle": "2024-01-07T16:55:45.379173Z",
     "shell.execute_reply": "2024-01-07T16:55:45.377605Z",
     "shell.execute_reply.started": "2024-01-07T16:55:44.330643Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory\n",
    "post_training_files_path = '/kaggle/working/runs/detect/train'\n",
    "\n",
    "# List the files in the directory\n",
    "!ls {post_training_files_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📁 Training Output Files Explained</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Here’s a rundown of each item:\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Weights Folder:</b> Contains the 'best.pt' and 'last.pt' files, which are the best and most recent weights of our trained model respectively.</li>\n",
    "        <li><b>Args:</b> A file that stores the arguments or parameters that were used during the training process.</li>\n",
    "        <li><b>Confusion Matrix:</b> Visual representations of the model performance. One is normalized, which helps in understanding the true positive rate across classes.</li>\n",
    "        <li><b>Events File:</b> Contains logs of events that occurred during training, useful for debugging and analysis.</li>\n",
    "        <li><b>F1 Curve:</b> Illustrates the F1 score of the model over time, balancing precision and recall.</li>\n",
    "        <li><b>Labels:</b> Shows the distribution of different classes within the dataset and their correlation.</li>\n",
    "        <li><b>P Curve, PR Curve, R Curve:</b> These are Precision, Precision-Recall, and Recall curves, respectively, providing insights into the trade-offs between different metrics at various thresholds.</li>\n",
    "        <li><b>results:</b> This csv file captures a comprehensive set of performance metrics recorded at each epoch during the model's training process.</li>\n",
    "        <li><b>Train Batch Images:</b> Sample images from the training set with model predictions overlaid, useful for a quick visual check of model performance.</li>\n",
    "        <li><b>Validation Batch Images:</b> Similar to train batch images, these are from the validation set and include both labels and predictions, providing a glimpse into how well the model generalizes.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        I will undertake a comprehensive evaluation and analysis of our model's performance, which involves:\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Learning Curves Analysis</b></li>\n",
    "        <li><b>Confusion Matrix Evaluation</b></li>\n",
    "        <li><b>Performance Metrics Assessment</b></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Learning_Curves\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 5.1 |</span><span style='color:#141140'> Learning Curves Analysis</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step, let's review the training and validation loss trends over epochs to assess the learning stability and efficiency:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.381771Z",
     "iopub.status.busy": "2024-01-07T16:55:45.381361Z",
     "iopub.status.idle": "2024-01-07T16:55:45.391793Z",
     "shell.execute_reply": "2024-01-07T16:55:45.390676Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.381734Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a function to plot learning curves for loss values\n",
    "def plot_learning_curve(df, train_loss_col, val_loss_col, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='#141140', linestyle='-', linewidth=2)\n",
    "    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='orangered', linestyle='--', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.393767Z",
     "iopub.status.busy": "2024-01-07T16:55:45.393389Z",
     "iopub.status.idle": "2024-01-07T16:55:46.863097Z",
     "shell.execute_reply": "2024-01-07T16:55:46.862056Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.393731Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the full file path for 'results.csv' using the directory path and file name\n",
    "results_csv_path = os.path.join(post_training_files_path, 'results.csv')\n",
    "\n",
    "# Load the CSV file from the constructed path into a pandas DataFrame\n",
    "df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Remove any leading whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Plot the learning curves for each loss\n",
    "plot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Box Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Model Learning Curve Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The learning curves for box loss, classification loss, and distribution focal loss indicate a rapid decrease in loss values during the initial epochs, leveling off as training progresses. This trend, along with the close alignment of training and validation loss lines, suggests that the model is learning effectively without overfitting, meaning it is well-tuned to the dataset without being biased or too variable.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The smoothness of the learning curves, especially evident in the latter epochs, implies that the model is reaching a state of equilibrium, where additional training does not significantly enhance performance. This observation suggests that 100 epochs are sufficient for training this model, as further training is unlikely to result in substantial gains.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Confusion_Matrix\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 5.2 |</span><span style='color:#141140'> Confusion Matrix Evaluation</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Next, I will focus on displaying and meticulously analyzing the confusion matrix derived from our model's performance on the validation dataset:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:46.865642Z",
     "iopub.status.busy": "2024-01-07T16:55:46.864744Z",
     "iopub.status.idle": "2024-01-07T16:55:48.072583Z",
     "shell.execute_reply": "2024-01-07T16:55:48.070774Z",
     "shell.execute_reply.started": "2024-01-07T16:55:46.865601Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Construct the path to the normalized confusion matrix image\n",
    "confusion_matrix_path = os.path.join(post_training_files_path, 'confusion_matrix_normalized.png')\n",
    "\n",
    "# Read the image using cv2\n",
    "cm_img = cv2.imread(confusion_matrix_path)\n",
    "\n",
    "# Convert the image from BGR to RGB color space for accurate color representation with matplotlib\n",
    "cm_img = cv2.cvtColor(cm_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10), dpi=120)\n",
    "plt.imshow(cm_img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Confusion Matrix Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The confusion matrix for our YOLOv8 vehicle detection model illustrates high accuracy as mentioned earlier as well. In 97% of instances, the model successfully identifies the presence of a vehicle when there is one, indicating strong detection capability. Conversely, in just 3% of cases, the model fails to detect a vehicle that is actually present, suggesting room for improvement in reducing false negatives.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Performance_Metrics\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 5.3 |</span><span style='color:#141140'> Performance Metrics Assessment</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Finally, I am delving into various metrics to understand the model's predictive accuracy and areas of potential improvement:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:48.078246Z",
     "iopub.status.busy": "2024-01-07T16:55:48.077923Z",
     "iopub.status.idle": "2024-01-07T16:56:00.887425Z",
     "shell.execute_reply": "2024-01-07T16:56:00.886243Z",
     "shell.execute_reply.started": "2024-01-07T16:55:48.07822Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Construct the path to the best model weights file using os.path.join\n",
    "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
    "\n",
    "# Load the best model weights into the YOLO model\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Validate the best model using the validation set with default parameters\n",
    "metrics = best_model.val(split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "as can be seen in the above verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.890082Z",
     "iopub.status.busy": "2024-01-07T16:56:00.88966Z",
     "iopub.status.idle": "2024-01-07T16:56:00.909676Z",
     "shell.execute_reply": "2024-01-07T16:56:00.908441Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.890045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
    "metrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 Model Evaluation Insights</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        The YOLOv8 model shows impressive results on the validation set. With a precision of <b>91.6%</b>, it indicates that the majority of the predictions made by the model are correct. The recall score of <b>93.8%</b> demonstrates the model's ability to find most of the relevant cases in the dataset. The model's mean Average Precision (mAP) at 50% Intersection over Union (IoU) is <b>97.5%</b>, reflecting high accuracy in detecting objects with a considerable overlap with the ground truth. Even when the IoU threshold range is expanded from 50% to 95%, the model maintains a solid mAP of <b>74.2%</b>. Finally, the fitness score of <b>76.5%</b> indicates a good balance between precision, recall, and the IoU of the predictions, confirming the model's effectiveness in object detection tasks.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Inference\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Model Inference & Generalization Assessment </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To thoroughly assess our model's capability to generalize, I will conduct inferences in three distinct steps:\n",
    "    </p>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Inference on Validation Set Images</b></li>\n",
    "        <li><b>Inference on an Unseen Test Image</b></li>\n",
    "        <li><b>Inference on an Unseen Test Video</b></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Validation\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 6.1 |</span><span style='color:#141140'> Inference on Validation Set Images</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> First of all, I am going to evaluate model predictions on random images from the validation dataset:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.911548Z",
     "iopub.status.busy": "2024-01-07T16:56:00.911155Z",
     "iopub.status.idle": "2024-01-07T16:56:04.95926Z",
     "shell.execute_reply": "2024-01-07T16:56:04.957846Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.911511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the validation images\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 9 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
    "\n",
    "# Initialize the subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
    "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
    "\n",
    "# Perform inference on each selected image and display it\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
    "    results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n",
    "    annotated_image = results[0].plot(line_width=1)\n",
    "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(annotated_image_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Test_Image\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 6.2 |</span><span style='color:#141140'> Inference on an Unseen Test Image</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Now, I will employ the best version of our fine-tuned model to evaluate its generalization capabilities. I'll test it on the same image previously analyzed using the pre-trained YOLOv8 model on the COCO dataset:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:04.961504Z",
     "iopub.status.busy": "2024-01-07T16:56:04.96104Z",
     "iopub.status.idle": "2024-01-07T16:56:06.324053Z",
     "shell.execute_reply": "2024-01-07T16:56:06.322966Z",
     "shell.execute_reply.started": "2024-01-07T16:56:04.961454Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "sample_image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
    "\n",
    "# Perform inference on the provided image using best model\n",
    "results = best_model.predict(source=sample_image_path, imgsz=640, conf=0.7) \n",
    "                        \n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv8 Model', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Enhanced Vehicle Detection with Fine-Tuning</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The comparison between the above image with the image we had in step 2 clearly demonstrates the benefits of fine-tuning the YOLOv8 model for a specialized task. In the image from step 2, the pre-trained model on the COCO dataset missed detecting a truck and misclassified it, indicating limitations when dealing with a specific class of objects due to its broader training scope. In contrast, the above image shows that the fine-tuned model on a vehicle-specific dataset has accurately detected and classified various vehicles, including the previously missed truck. This improvement highlights the model's enhanced capability to discern features specific to vehicles, leading to better precision and recall in vehicle detection.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Test_Video\"></a>\n",
    "# <b><span style='color:#b2addb'>Step 6.3 |</span><span style='color:#141140'> Inference on an Unseen Test Video</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Finally, I am going to assess the model's generalization capabilities on a completely new video, unseen during training. This step is crucial to demonstrate the model's ability to adapt and perform accurately in real-world applications, further solidifying its effectiveness outside of the controlled dataset environment:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:06.325982Z",
     "iopub.status.busy": "2024-01-07T16:56:06.325408Z",
     "iopub.status.idle": "2024-01-07T16:56:25.874908Z",
     "shell.execute_reply": "2024-01-07T16:56:25.873977Z",
     "shell.execute_reply.started": "2024-01-07T16:56:06.32595Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the sample video in the dataset\n",
    "dataset_video_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'\n",
    "\n",
    "# Define the destination path in the working directory\n",
    "video_path = '/kaggle/working/sample_video.mp4'\n",
    "\n",
    "# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\n",
    "shutil.copyfile(dataset_video_path, video_path)\n",
    "\n",
    "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
    "best_model.predict(source=video_path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To ensure compatibility with various platforms, including Jupyter Notebooks, we'll convert the output <code>.avi</code> video file to the more universally supported <code>.mp4</code> format and then display it within the notebook environment:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:25.876607Z",
     "iopub.status.busy": "2024-01-07T16:56:25.876267Z",
     "iopub.status.idle": "2024-01-07T16:56:40.862642Z",
     "shell.execute_reply": "2024-01-07T16:56:40.860731Z",
     "shell.execute_reply.started": "2024-01-07T16:56:25.876578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/sample_video.avi processed_sample_video.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"processed_sample_video.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Traffic_Intensity_Estimation\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Real-Time Traffic Intensity Estimation </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        As we transition into the practical application step of our project, we are set to deploy our finely-tuned vehicle detection model to analyze traffic density. This step is crucial in demonstrating the model's ability to generalize and perform accurately on unseen videos—videos that were not part of the model's training or validation sets.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "Our objective is to quantify the traffic by counting vehicles within specified areas on the road lanes, frame by frame. The analysis will not only reveal the vehicle count but also gauge the intensity of traffic, labeling it as 'Heavy' or 'Smooth' based on a predetermined threshold. The count and traffic flow insights are pivotal for urban planning and traffic management.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "While real-time processing on Kaggle is not viable, the code below simulates this process by modifying video frames, applying vehicle detection, and annotating the results. This mimics real-time analysis which can be achieved on local machines—even on CPUs—by processing webcam feeds or video files in real-time. The annotated video is then saved, ready to be reviewed for traffic assessment.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "Let's delve into the code that brings this all to life:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:40.865451Z",
     "iopub.status.busy": "2024-01-07T16:56:40.865105Z",
     "iopub.status.idle": "2024-01-07T16:56:55.842674Z",
     "shell.execute_reply": "2024-01-07T16:56:55.841793Z",
     "shell.execute_reply.started": "2024-01-07T16:56:40.865421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the threshold for considering traffic as heavy\n",
    "heavy_traffic_threshold = 10\n",
    "\n",
    "# Define the vertices for the quadrilaterals\n",
    "vertices1 = np.array([(465, 350), (609, 350), (510, 630), (2, 630)], dtype=np.int32)\n",
    "vertices2 = np.array([(678, 350), (815, 350), (1203, 630), (743, 630)], dtype=np.int32)\n",
    "\n",
    "# Define the vertical range for the slice and lane threshold\n",
    "x1, x2 = 325, 635 \n",
    "lane_threshold = 609\n",
    "\n",
    "# Define the positions for the text annotations on the image\n",
    "text_position_left_lane = (10, 50)\n",
    "text_position_right_lane = (820, 50)\n",
    "intensity_position_left_lane = (10, 100)\n",
    "intensity_position_right_lane = (820, 100)\n",
    "\n",
    "# Define font, scale, and colors for the annotations\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (255, 255, 255)    # White color for text\n",
    "background_color = (0, 0, 255)  # Red background for text\n",
    "        \n",
    "# Open the video\n",
    "cap = cv2.VideoCapture('sample_video.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('traffic_density_analysis.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Read until video is completed\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Create a copy of the original frame to modify\n",
    "        detection_frame = frame.copy()\n",
    "    \n",
    "        # Black out the regions outside the specified vertical range\n",
    "        detection_frame[:x1, :] = 0  # Black out from top to x1\n",
    "        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n",
    "        \n",
    "        # Perform inference on the modified frame\n",
    "        results = best_model.predict(detection_frame, imgsz=640, conf=0.4)\n",
    "        processed_frame = results[0].plot(line_width=1)\n",
    "        \n",
    "        # Restore the original top and bottom parts of the frame\n",
    "        processed_frame[:x1, :] = frame[:x1, :].copy()\n",
    "        processed_frame[x2:, :] = frame[x2:, :].copy()        \n",
    "        \n",
    "        # Draw the quadrilaterals on the processed frame\n",
    "        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        # Retrieve the bounding boxes from the results\n",
    "        bounding_boxes = results[0].boxes\n",
    "\n",
    "        # Initialize counters for vehicles in each lane\n",
    "        vehicles_in_left_lane = 0\n",
    "        vehicles_in_right_lane = 0\n",
    "\n",
    "        # Loop through each bounding box to count vehicles in each lane\n",
    "        for box in bounding_boxes.xyxy:\n",
    "            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n",
    "            if box[0] < lane_threshold:\n",
    "                vehicles_in_left_lane += 1\n",
    "            else:\n",
    "                vehicles_in_right_lane += 1\n",
    "                \n",
    "        # Determine the traffic intensity for the left lane\n",
    "        traffic_intensity_left = \"Heavy\" if vehicles_in_left_lane > heavy_traffic_threshold else \"Smooth\"\n",
    "        # Determine the traffic intensity for the right lane\n",
    "        traffic_intensity_right = \"Heavy\" if vehicles_in_right_lane > heavy_traffic_threshold else \"Smooth\"\n",
    "\n",
    "\n",
    "        # Add a background rectangle for the left lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25), \n",
    "                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the left lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25), \n",
    "                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25), \n",
    "                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25), \n",
    "                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n",
    "        # cv2.imshow('Real-time Analysis', processed_frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n",
    "        #     break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all the frames\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Finally, lets convert the output <code>.avi</code> video to <code>.mp4</code> format for notebook playback and display it:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:55.844457Z",
     "iopub.status.busy": "2024-01-07T16:56:55.844123Z",
     "iopub.status.idle": "2024-01-07T16:57:08.599235Z",
     "shell.execute_reply": "2024-01-07T16:57:08.597866Z",
     "shell.execute_reply.started": "2024-01-07T16:56:55.844428Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /kaggle/working/traffic_density_analysis.avi traffic_density_analysis.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"traffic_density_analysis.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Export\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 8 | Model Export for Cross-Platform Deployment </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        We're now at the final step of our project – exporting our best model for future use and deployment. Currently, our model exists in a <code>.pt</code> format, optimal for tasks such as ongoing training, additional fine-tuning, or deployment within PyTorch-compatible environments. To enhance the model's versatility and ensure it can be utilized across various platforms, we'll be exporting it in the ONNX (Open Neural Network Exchange) format. ONNX is specially designed for model portability, enabling seamless operation across different machine learning frameworks including PyTorch, TensorFlow, and Microsoft's Cognitive Toolkit (CNTK), as well as compatibility with specialized hardware accelerators. This step ensures our model's readiness for diverse deployment scenarios, aligning with the broad scope of modern AI applications. For more details on the export process and available formats, refer to <a href=\"https://docs.ultralytics.com/modes/export/#arguments\">Ultralytics Documentation</a>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:57:08.601218Z",
     "iopub.status.busy": "2024-01-07T16:57:08.600853Z",
     "iopub.status.idle": "2024-01-07T16:57:10.881191Z",
     "shell.execute_reply": "2024-01-07T16:57:10.880392Z",
     "shell.execute_reply.started": "2024-01-07T16:57:08.601188Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "best_model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #eae8fa; font-size: 120%; text-align: center;\">\n",
    "    <strong>\n",
    "        🌐 For comprehensive insights, extensive code, and additional resources, visit the project's \n",
    "        <a href=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/tree/master\" style=\"color: #141140; text-decoration: none;\">\n",
    "            <em><u>GitHub Repository</u></em>\n",
    "        </a> \n",
    "        🌐\n",
    "    </strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color=#141140>Best Regards</font></h2>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4107330,
     "sourceId": 7166601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 68.691048,
   "end_time": "2023-12-12T12:45:53.82776",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T12:44:45.136712",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
