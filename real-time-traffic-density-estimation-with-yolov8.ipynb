{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017921,
     "end_time": "2023-12-12T12:44:48.731519",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.713598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/cover_image.png?raw=true\" width=\"2400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 目标检测概述</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"><strong>目标检测</strong> 是一种识别图像或视频流中物体位置和类别的技术。它的输出包括每个物体的边界框（用于包围物体）、类别标签以及置信度分数。当不需要精确的物体形状细节时，这种方法非常适合快速定位场景中的物体。目前，用于此任务的主要模型包括 <strong>RCNN</strong>、<strong>SSD</strong> 和 <strong>YOLO</strong>。其中，YOLO 模型表现尤为出色，尤其在实时检测场景中，它在速度和精度的平衡上优于其他模型。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚀 YOLO：实时目标检测革新方法</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <strong>YOLO</strong>（即 “<strong>You Only Look Once</strong>” 的缩写）通过提供一种快速且高效的方法，彻底改变了计算机视觉中的目标检测方式。它将目标定位和分类结合为一步操作，极大地简化了检测流程。这种创新方法使YOLO能够以高精度实时处理图像和视频。最新版本的<strong>YOLOv8</strong>进一步提升了这项技术，能够出色地应对目标检测和图像分割中的挑战。YOLO的实时能力使其成为需要快速精准识别目标的应用场景中的首选。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🌟 YOLOv8：目标检测技术的前沿突破</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <strong>YOLOv8</strong> 由 <strong>Ultralytics</strong> 在 <strong>2023年1月</strong> 推出，是YOLO系列人工智能模型的最新成果。YOLOv8适用于分类、目标检测和图像分割等任务，其精度和速度都优于前代产品YOLOv7。该模型以<strong>Darknet53</strong>为骨干网络，使用更多的特征图和高效的卷积神经网络，从而实现了更高的平均精度（mAP）和每秒帧数（fps）。YOLOv8引入了一种创新的无锚点检测头，能够像图像分割技术一样进行像素级边界框估计，并采用了新的损失函数。这些特性的结合使YOLOv8在<strong>COCO基准数据集</strong>上达到了<strong>53.7%</strong>的平均精度（<strong>mean Average Precision</strong>），使其成为目标检测领域的顶尖模型。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🚦 交通流量估计项目概述</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        借助YOLO的实时检测能力，本项目专注于<strong>交通流量估计</strong>，这是城市和交通管理中的一个重要组成部分。项目的目标是在每一帧图像中统计特定区域内的车辆数量，以评估交通流量。这些有价值的数据有助于识别交通高峰时段、拥堵区域，并协助城市规划。通过本项目，我们旨在开发一套全面的工具，提供关于交通流动和模式的详细见解，以增强交通管理和城市规划策略。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🎯 项目目标</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><strong>YOLOv8模型选择与初步评估：</strong> 从YOLOv8的预训练模型开始，评估其在COCO数据集上对车辆检测的初始性能。</li>\n",
    "        <li><strong>专业车辆数据集准备：</strong> 整理和标注专门的车辆数据集，以优化模型对多种车辆类型的检测能力。</li>\n",
    "        <li><strong>模型微调以提升车辆检测性能：</strong> 使用迁移学习对YOLOv8模型进行微调，专注于从空中视角检测车辆，以提高检测的精确度和召回率。</li>\n",
    "        <li><strong>全面评估模型性能：</strong> 分析学习曲线，评估混淆矩阵，并评估性能指标，以验证模型的准确性和泛化能力。</li>\n",
    "        <li><strong>在测试数据上进行推理和泛化：</strong> 在验证图像、未见过的测试图像和测试视频上测试模型的泛化能力，以展示其实用性和有效性。</li>\n",
    "        <li><strong>实时交通流量估计：</strong> 实现一种算法，通过在测试视频数据上实时统计车辆数量和分析交通强度，来估计交通流量。</li>\n",
    "        <li><strong>跨平台模型部署准备：</strong> 将微调后的模型导出为ONNX格式，以便在不同平台和环境中灵活使用。</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016739,
     "end_time": "2023-12-12T12:44:48.938242",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.921503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"contents_tabel\"></a>   \n",
    "\n",
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📋 Table of Contents</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><a href=\"#Initialization\" style=\"text-decor\n",
    "            ation: none;\">Step 1 | Setup and Initialization</a></li>\n",
    "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">Step 2 | Loading YOLOv8 Pre-trained Model</a></li>\n",
    "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">Step 3 | Dataset Exploration</a></li>\n",
    "        <li><a href=\"#Fine_Tuning_YOLOv8\" style=\"text-decoration: none;\">Step 4 | Fine-Tuning YOLOv8</a></li>\n",
    "        <li><a href=\"#Model_Performance_Evaluation\" style=\"text-decoration: none;\">Step 5 | Model Performance Evaluation</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">Step 5.1 | Learning Curves Analysis</a></li>\n",
    "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">Step 5.2 | Confusion Matrix Evaluation</a></li>\n",
    "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">Step 5.3 | Performance Metrics Assessment</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">Step 6 | Model Inference & Generalization Assessment</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Inference_Validation\" style=\"text-decoration: none;\">Step 6.1 | Inference on Validation Set Images</a></li>\n",
    "                <li><a href=\"#Inference_Test_Image\" style=\"text-decoration: none;\">Step 6.2 | Inference on an Unseen Test Image</a></li>\n",
    "                <li><a href=\"#Inference_Test_Video\" style=\"text-decoration: none;\">Step 6.3 | Inference on an Unseen Test Video</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Traffic_Intensity_Estimation\" style=\"text-decoration: none;\">Step 7 | Real-Time Traffic Intensity Estimation</a></li>\n",
    "        <li><a href=\"#Model_Export\" style=\"text-decoration: none;\">Step 8 | Model Export for Cross-Platform Deployment</a></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents_table\"></a>   \n",
    "\n",
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 15px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📋 目录</b></h1>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><a href=\"#Initialization\" style=\"text-decoration: none;\">步骤 1 | 环境设置与初始化</a></li>\n",
    "        <li><a href=\"#Load_Model\" style=\"text-decoration: none;\">步骤 2 | 加载 YOLOv8 预训练模型</a></li>\n",
    "        <li><a href=\"#Dataset_Exploration\" style=\"text-decoration: none;\">步骤 3 | 数据集探索</a></li>\n",
    "        <li><a href=\"#Fine_Tuning_YOLOv8\" style=\"text-decoration: none;\">步骤 4 | YOLOv8 微调</a></li>\n",
    "        <li><a href=\"#Model_Performance_Evaluation\" style=\"text-decoration: none;\">步骤 5 | 模型性能评估</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Learning_Curves\" style=\"text-decoration: none;\">步骤 5.1 | 学习曲线分析</a></li>\n",
    "                <li><a href=\"#Confusion_Matrix\" style=\"text-decoration: none;\">步骤 5.2 | 混淆矩阵评估</a></li>\n",
    "                <li><a href=\"#Performance_Metrics\" style=\"text-decoration: none;\">步骤 5.3 | 性能指标评估</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Model_Inference\" style=\"text-decoration: none;\">步骤 6 | 模型推理与泛化能力评估</a>\n",
    "            <ul>\n",
    "                <li><a href=\"#Inference_Validation\" style=\"text-decoration: none;\">步骤 6.1 | 验证集图像推理</a></li>\n",
    "                <li><a href=\"#Inference_Test_Image\" style=\"text-decoration: none;\">步骤 6.2 | 未见测试图像推理</a></li>\n",
    "                <li><a href=\"#Inference_Test_Video\" style=\"text-decoration: none;\">步骤 6.3 | 未见测试视频推理</a></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#Traffic_Intensity_Estimation\" style=\"text-decoration: none;\">步骤 7 | 实时交通流量估计</a></li>\n",
    "        <li><a href=\"#Model_Export\" style=\"text-decoration: none;\">步骤 8 | 模型导出以实现跨平台部署</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017755,
     "end_time": "2023-12-12T12:44:48.973007",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.955252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color=#141140>Let's get started:</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016921,
     "end_time": "2023-12-12T12:44:49.006952",
     "exception": false,
     "start_time": "2023-12-12T12:44:48.990031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Initialization\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤一 | 环境设置与初始化</p>\n",
    "⬆️ [返回主目录](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017387,
     "end_time": "2023-12-12T12:44:49.08451",
     "exception": false,
     "start_time": "2023-12-12T12:44:49.067123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">YOLOv8 is the first version of the YOLO series to offer an official package that can be easily installed using pip. This streamlines the setup process, a notable advancement from previous versions that required cloning repositories and running scripts. Let's begin by installing the Ultralytics package:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">YOLOv8是YOLO系列中首个提供官方软件包的版本，可以通过pip轻松安装。这大大简化了设置过程，与之前需要克隆仓库并运行脚本的版本相比，这是一个显著的进步。我们先从安装Ultralytics软件包开始：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:14.811943Z",
     "iopub.status.busy": "2024-01-07T16:41:14.81127Z",
     "iopub.status.idle": "2024-01-07T16:41:28.603129Z",
     "shell.execute_reply": "2024-01-07T16:41:28.602158Z",
     "shell.execute_reply.started": "2024-01-07T16:41:14.811904Z"
    },
    "papermill": {
     "duration": 13.887399,
     "end_time": "2023-12-12T12:45:02.989658",
     "exception": false,
     "start_time": "2023-12-12T12:44:49.102259",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install Ultralytics library\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018229,
     "end_time": "2023-12-12T12:45:03.027025",
     "exception": false,
     "start_time": "2023-12-12T12:45:03.008796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">接下来，导入项目所需的全部重要库：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:28.605394Z",
     "iopub.status.busy": "2024-01-07T16:41:28.605102Z",
     "iopub.status.idle": "2024-01-07T16:41:33.110352Z",
     "shell.execute_reply": "2024-01-07T16:41:33.109382Z",
     "shell.execute_reply.started": "2024-01-07T16:41:28.605368Z"
    },
    "papermill": {
     "duration": 5.616125,
     "end_time": "2023-12-12T12:45:08.66149",
     "exception": false,
     "start_time": "2023-12-12T12:45:03.045365",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Disable warnings in the notebook to maintain clean output cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.111939Z",
     "iopub.status.busy": "2024-01-07T16:41:33.111517Z",
     "iopub.status.idle": "2024-01-07T16:41:33.116783Z",
     "shell.execute_reply": "2024-01-07T16:41:33.115938Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.111912Z"
    },
    "papermill": {
     "duration": 0.027946,
     "end_time": "2023-12-12T12:45:08.709398",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.681452",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure the visual appearance of Seaborn plots\n",
    "sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019009,
     "end_time": "2023-12-12T12:45:08.747327",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.728318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Load_Model\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤 2 | 加载 YOLOv8 预训练模型</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018944,
     "end_time": "2023-12-12T12:45:08.785891",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.766947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Here are the pre-trained YOLOv8 object detection models, which have been trained on the <strong>COCO dataset</strong>. The Common Objects in Context (COCO) dataset is extensive, designed for object detection, segmentation, and captioning, and encompasses <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80 diverse object categories</a>:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">这里提供了在 <strong>COCO数据集</strong> 上预训练的YOLOv8目标检测模型。COCO（Common Objects in Context）数据集内容丰富，专为物体检测、分割和标注设计，涵盖了 <a href=\"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml\">80种不同的物体类别</a>。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018452,
     "end_time": "2023-12-12T12:45:08.823299",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.804847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/blob/master/images/YOLOv8_object_detection_models.jpg?raw=true\" width=\"2400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018195,
     "end_time": "2023-12-12T12:45:08.860376",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.842181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📈 Model Performance Trade-offs</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The YOLOv8 suite presents five distinct models: <strong>nano</strong>, <strong>small</strong>, <strong>medium</strong>, <strong>large</strong>, and <strong>xlarge</strong>. A clear trend emerges from the data: as model size increases, there's a notable improvement in <strong>mAP</strong>, indicating enhanced accuracy. Conversely, this augmentation comes at the cost of speed, with larger models being slower. All models adhere to a standard input size of <strong>640x640</strong> pixels, optimizing performance across diverse applications.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>📈 模型性能权衡</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        YOLOv8系列提供了五种不同型号的模型：<strong>微型</strong>（nano）、<strong>小型</strong>（small）、<strong>中型</strong>（medium）、<strong>大型</strong>（large）和<strong>超大型</strong>（xlarge）。数据显示出一个明显的趋势：随着模型尺寸的增大，<strong>mAP</strong>（平均精度）有显著提升，这意味着更高的准确性。然而，这种提升是以速度为代价的，因为更大的模型运行速度更慢。所有模型都采用统一的输入尺寸<strong>640x640</strong>像素，以优化在不同应用中的性能。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018694,
     "end_time": "2023-12-12T12:45:08.897562",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.878868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Intersection Over Union (IoU)</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        IoU is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between the predicted bounding box and the ground truth, with values ranging from 0 (no overlap) to 1 (perfect overlap). IoU is crucial for determining whether a detection is a true positive or a false positive, often using a threshold like 0.5 or 0.75 to make this distinction.\n",
    "    </p>\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 Mean Average Precision (mAP)</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        mAP is a commonly used metric to evaluate the precision of object detection models. It is the average of the AP (Average Precision) calculated for all the classes and is based on the area under the precision-recall curve. This metric reflects the model's precision across different levels of recall, providing a comprehensive performance measure that accounts for both the detection accuracy and the ability to detect all relevant objects.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 交并比（IoU）</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        交并比（IoU）是用于评估目标检测器在特定数据集上准确性的指标。它衡量预测边界框与真实边界框之间的重叠程度，取值范围从0（无重叠）到1（完全重叠）。IoU对于判断检测结果是真正例还是假正例至关重要，通常使用0.5或0.75这样的阈值来进行区分。\n",
    "    </p>\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🎯 平均精度均值（mAP）</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        平均精度均值（mAP）是评估目标检测模型精度的常用指标。它是所有类别平均精度（AP）的平均值，基于精确率-召回率曲线下的面积计算。这一指标反映了模型在不同召回率水平下的精确率，提供了一个综合衡量模型性能的指标，既考虑了检测的准确性，也考虑了检测所有相关目标的能力。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018603,
     "end_time": "2023-12-12T12:45:08.935075",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.916472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">For our real-time traffic density estimation application, I am going to select the <strong>YOLOv8 nano pre-trained model (yolov8n.pt)</strong> to handle vehicle detection. This model ensures the fastest possible inference time, making it well-suited for real-time use:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">在本项目实时交通流量估计应用中，将选择<strong>YOLOv8微型预训练模型（yolov8n.pt）</strong>来处理车辆检测。该模型确保了最快的推理时间，非常适合实时使用。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:33.118523Z",
     "iopub.status.busy": "2024-01-07T16:41:33.118193Z",
     "iopub.status.idle": "2024-01-07T16:41:34.004717Z",
     "shell.execute_reply": "2024-01-07T16:41:34.00379Z",
     "shell.execute_reply.started": "2024-01-07T16:41:33.118492Z"
    },
    "papermill": {
     "duration": 0.536317,
     "end_time": "2023-12-12T12:45:09.490231",
     "exception": false,
     "start_time": "2023-12-12T12:45:08.953914",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model from Ultralytics\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018601,
     "end_time": "2023-12-12T12:45:09.528765",
     "exception": false,
     "start_time": "2023-12-12T12:45:09.510164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">The pre-trained model we've loaded is trained on the COCO dataset, which includes the 'car' and 'truck' classes among its 80 different categories — exactly what we need for our project. Now, let's put our model to the test and see how it performs on a sample image:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">上面加载的预训练模型是在COCO数据集上训练的，其中包含80个不同类别中的“汽车”和“卡车”类别——这正是我们项目所需要的。下面将对模型进行测试，看看它在样本图像上的表现如何：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:34.008052Z",
     "iopub.status.busy": "2024-01-07T16:41:34.007757Z",
     "iopub.status.idle": "2024-01-07T16:41:36.773138Z",
     "shell.execute_reply": "2024-01-07T16:41:36.771864Z",
     "shell.execute_reply.started": "2024-01-07T16:41:34.008028Z"
    },
    "papermill": {
     "duration": 8.967725,
     "end_time": "2023-12-12T12:45:18.515569",
     "exception": false,
     "start_time": "2023-12-12T12:45:09.547844",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
    "\n",
    "# Perform inference on the provided image(s)\n",
    "results = model.predict(source=image_path, \n",
    "                        imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n",
    "                        conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n",
    "\n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04642,
     "end_time": "2023-12-12T12:45:18.611612",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.565192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Pre-trained Model Detection Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        In our sample image, the pre-trained model missed the detectable truck and car that were clearly visible. A model pre-trained on a dataset with a broad range of classes, like COCO's 80 different categories, may not perform as well on a specific subset of those categories due to the diversity of objects it has been trained to recognize. If we fine-tune this model on a specialized dataset that focuses solely on vehicles, it can learn to detect various types of vehicles more accurately. Fine-tuning on a vehicle-specific dataset allows the model to become more specialized, adjusting the weights to be more sensitive to features specific to vehicles. As a result, the model's mean Average Precision (mAP) for vehicle detection could improve because it's being optimized on a narrower, more relevant range of classes for our specific application. Fine-tuning also helps the model generalize better for vehicle detection tasks, potentially reducing false negatives (like missing a detectable truck) and improving overall detection performance.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 预训练模型检测分析</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        在我们的样本图像中，预训练模型遗漏了明显可见的卡车和汽车。在像COCO这样的包含80个不同类别的广泛数据集上预训练的模型，由于其训练识别的物体种类繁多，在特定子类别上的表现可能不如在专门针对车辆的数据集上训练的模型[^14^]。如果我们在这个模型上针对专门的车辆数据集进行微调，它可以更准确地检测各种类型的车辆。针对车辆的专门数据集进行微调可以使模型变得更加专业化，调整权重以对车辆的特定特征更加敏感[^14^]。因此，模型在车辆检测上的平均精度均值（mAP）可能会提高，因为它正在针对我们特定应用的相关性更窄的类别范围进行优化[^15^]。微调还有助于模型更好地泛化车辆检测任务，可能会减少假阴性（如遗漏可检测的卡车）并提高整体检测性能[^14^]。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046519,
     "end_time": "2023-12-12T12:45:18.70644",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.659921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Dataset_Exploration\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤三 | 数据集探索</p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04706,
     "end_time": "2023-12-12T12:45:18.800921",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.753861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Dataset Preparation for Model Fine-tuning</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To fine-tune our pre-trained model on a specialized dataset that focuses solely on vehicles, so that it can learn to detect various types of vehicles more accurately, I have prepared a dataset which is available at this link <a href=\"https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset\">Top-View Vehicle Detection Image Dataset for YOLOv8</a>. The dataset zeroes in on the 'Vehicle' class, covering a wide variety of vehicles such as cars, trucks, and buses. It is composed of 626 images sourced from top-view perspectives, <strong>annotated meticulously in the YOLOv8 format</strong> for effective vehicle detection.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The dataset undergoes a standardization process where each image is resized to a uniform resolution of <strong>640x640 pixels</strong>. To bolster the model's ability to generalize, augmentations were applied to the training data, which consists of 536 images. The validation set contains 90 images and remains unaugmented to preserve the integrity of performance evaluation.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        This dataset, curated from <a href=\"https://www.pexels.com/search/videos/\">Pexels</a>, captures the diversity of vehicles from an aerial view, making it ideal for highway monitoring tasks. Each video frame was selected at a sampling rate of 1 frame per second using <a href=\"https://universe.roboflow.com/farzad/vehicle_detection_yolov8\">Roboflow</a>, which facilitated precise annotation for object detection.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 为模型微调准备数据集</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        为了在专门针对车辆的数据集上微调我们的预训练模型，使其能够更准确地检测各种类型的车辆，这里准备了一个数据集，数据集可通过此链接访问：<a href=\"https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset\">Top-View Vehicle Detection Image Dataset for YOLOv8</a>。该数据集专注于“车辆”类别，涵盖了汽车、卡车和公交车等多种类型的车辆。数据集包含626张图像，这些图像均来自顶部视角，并且已经<strong>以YOLOv8格式进行了标注</strong>，以便进行有效的车辆检测。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        数据集经过标准化处理，每张图像都被调整为统一的分辨率<strong>640x640像素</strong>。为了增强模型的泛化能力，我们对训练数据（包含536张图像）进行了增强处理。验证集包含90张图像，未进行增强处理，以保持性能评估的完整性。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        该数据集是从<a href=\"https://www.pexels.com/search/videos/\">Pexels</a>精心挑选的，从空中视角捕捉了车辆的多样性，非常适合用于高速公路监控任务。每个视频帧都是通过<a href=\"https://universe.roboflow.com/farzad/vehicle_detection_yolov8\">Roboflow</a>以每秒1帧的采样率选取的，这有助于进行精确的目标检测标注。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046033,
     "end_time": "2023-12-12T12:45:18.893777",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.847744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ YOLOv8 Dataset Format</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Our dataset, structured for YOLOv8 format, has been meticulously prepared on Roboflow. It encompasses all necessary components for an efficient object detection model training. Here’s a detailed breakdown:\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>1️⃣ Train Directory:</b><br>\n",
    "        The 'train' directory houses our training dataset. It includes 536 images within the 'images' subfolder and corresponding YOLOv8 format labels in the 'labels' subfolder.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>2️⃣ Validation Directory:</b><br>\n",
    "        The 'valid' directory contains the validation dataset. This consists of 90 images in the 'images' subfolder and their respective YOLOv8 format labels in the 'labels' subfolder.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>3️⃣ data.yaml:</b><br>\n",
    "        This file is the Ultralytics YOLO dataset configuration file. It specifies paths to the training and validation datasets, defines the number of classes (1), and the class name ('Vehicle'). This format is crucial for setting up and training the model accurately with our dataset.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>📝 Note about labels:</b><br>\n",
    "        Labels in our dataset are formatted in YOLO style, where each image is associated with a *.txt file. These files describe the detected objects in a '<strong>class x_center y_center width height</strong>' format. Importantly, the box coordinates are normalized between 0 and 1. If an image has no detectable objects, it won’t have a corresponding *.txt file.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ YOLOv8 数据集格式</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        我们的数据集已按照YOLOv8格式在Roboflow上精心准备，涵盖了高效目标检测模型训练所需的所有必要组件。以下是详细说明：\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>1️⃣ 训练目录：</b><br>\n",
    "        “train”目录存放我们的训练数据集。其中包含536张图像，位于“images”子文件夹中，对应的YOLOv8格式标签位于“labels”子文件夹中。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>2️⃣ 验证目录：</b><br>\n",
    "        “valid”目录包含验证数据集。其中包含90张图像，位于“images”子文件夹中，对应的YOLOv8格式标签位于“labels”子文件夹中。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>3️⃣ data.yaml：</b><br>\n",
    "        该文件是Ultralytics YOLO数据集配置文件。它指定了训练和验证数据集的路径，定义了类别数量（1个）以及类别名称（“Vehicle”）。这种格式对于使用我们的数据集准确设置和训练模型至关重要。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>📝 关于标签的说明：</b><br>\n",
    "        我们数据集中的标签采用YOLO风格格式，每张图像都关联一个*.txt文件。这些文件以“<strong>类别 x_center y_center width height</strong>”的格式描述检测到的物体。重要的是，框坐标被归一化在0到1之间。如果图像中没有可检测的物体，则不会生成对应的*.txt文件。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046651,
     "end_time": "2023-12-12T12:45:18.986753",
     "exception": false,
     "start_time": "2023-12-12T12:45:18.940102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's begin our exploration by examining the '<strong>data.yaml</strong>' file:</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">让我们先从查看“<strong>data.yaml</strong>”文件开始探索：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.774876Z",
     "iopub.status.busy": "2024-01-07T16:41:36.774521Z",
     "iopub.status.idle": "2024-01-07T16:41:36.788027Z",
     "shell.execute_reply": "2024-01-07T16:41:36.787189Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.774845Z"
    },
    "papermill": {
     "duration": 0.066253,
     "end_time": "2023-12-12T12:45:19.099731",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.033478",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the dataset_path\n",
    "dataset_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset'\n",
    "\n",
    "# Set the path to the YAML file\n",
    "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
    "\n",
    "# Load and print the contents of the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(yaml.dump(yaml_content, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046159,
     "end_time": "2023-12-12T12:45:19.194321",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.148162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ Understanding the data.yaml File</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        As I previously mentioned, the 'data.yaml' file is a key part of setting up our model. It points out where to find the training and validation images and tells the model that we're focusing on just one class, named 'Vehicle'. This file is essential for making sure our Ultralytics YOLOv8 model learns from our specific dataset, as it guides the model to understand exactly what it needs to look for and where.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🗂️ 理解 data.yaml 文件</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        正如之前提到的，'data.yaml' 文件是我们设置模型的关键部分。它指定了在哪里可以找到训练和验证图像，并告诉模型我们只关注一个类别，名为 “Vehicle”。这个文件对于确保我们的 Ultralytics YOLOv8 模型从特定的数据集学习至关重要，因为它指导模型准确地了解它需要查找的内容和位置。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046625,
     "end_time": "2023-12-12T12:45:19.287516",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.240891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Now, let's continue our exploration by counting the images in both the training and validation sets and verifying their sizes:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">接下来，我们继续探索，统计训练集和验证集中的图像数量并验证它们的尺寸：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:36.789658Z",
     "iopub.status.busy": "2024-01-07T16:41:36.789244Z",
     "iopub.status.idle": "2024-01-07T16:41:39.11087Z",
     "shell.execute_reply": "2024-01-07T16:41:39.109926Z",
     "shell.execute_reply.started": "2024-01-07T16:41:36.789629Z"
    },
    "papermill": {
     "duration": 4.275063,
     "end_time": "2023-12-12T12:45:23.611095",
     "exception": false,
     "start_time": "2023-12-12T12:45:19.336032",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set paths for training and validation image sets\n",
    "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# Initialize counters for the number of images\n",
    "num_train_images = 0\n",
    "num_valid_images = 0\n",
    "\n",
    "# Initialize sets to hold the unique sizes of images\n",
    "train_image_sizes = set()\n",
    "valid_image_sizes = set()\n",
    "\n",
    "# Check train images sizes and count\n",
    "for filename in os.listdir(train_images_path):\n",
    "    if filename.endswith('.jpg'):  \n",
    "        num_train_images += 1\n",
    "        image_path = os.path.join(train_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            train_image_sizes.add(img.size)\n",
    "\n",
    "# Check validation images sizes and count\n",
    "for filename in os.listdir(valid_images_path):\n",
    "    if filename.endswith('.jpg'): \n",
    "        num_valid_images += 1\n",
    "        image_path = os.path.join(valid_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            valid_image_sizes.add(img.size)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of validation images: {num_valid_images}\")\n",
    "\n",
    "# Check if all images in training set have the same size\n",
    "if len(train_image_sizes) == 1:\n",
    "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Training images have varying sizes.\")\n",
    "\n",
    "# Check if all images in validation set have the same size\n",
    "if len(valid_image_sizes) == 1:\n",
    "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Validation images have varying sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051006,
     "end_time": "2023-12-12T12:45:23.712196",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.66119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📊 Dataset Analysis Insights</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The dataset for our project consists of 536 training images and 90 validation images, all uniformly sized at 640x640 pixels. This size aligns with the benchmark standard for the YOLOv8 model, ensuring optimal accuracy and speed during model performance. The split ratio of approximately 85% for training and 15% for validation provides a substantial amount of data for model learning while retaining enough images for effective model validation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📊 数据集分析洞察</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        我们项目的数据集包含536张训练图像和90张验证图像，所有图像的尺寸均为640x640像素。这一尺寸与YOLOv8模型的基准标准一致，确保了模型在性能表现上的最佳准确性和速度。训练集和验证集的划分比例约为85%和15%，为模型学习提供了充足的数据量，同时也保留了足够多的图像用于有效的模型验证。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047477,
     "end_time": "2023-12-12T12:45:23.809043",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.761566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Let's take a look at a few images from the training dataset to get a sense of what the data looks like:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">下面查看训练数据集中的几张图像，以了解这些数据看起来是什么样的：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:39.112348Z",
     "iopub.status.busy": "2024-01-07T16:41:39.112057Z",
     "iopub.status.idle": "2024-01-07T16:41:41.729949Z",
     "shell.execute_reply": "2024-01-07T16:41:41.728821Z",
     "shell.execute_reply.started": "2024-01-07T16:41:39.112322Z"
    },
    "papermill": {
     "duration": 2.758546,
     "end_time": "2023-12-12T12:45:26.614041",
     "exception": false,
     "start_time": "2023-12-12T12:45:23.855495",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 8 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n",
    "\n",
    "# Create a 2x4 subplot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 11))\n",
    "\n",
    "# Display each of the selected images\n",
    "for ax, img_file in zip(axes.ravel(), selected_images):\n",
    "    img_path = os.path.join(train_images_path, img_file)\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.suptitle('Sample Images from Training Dataset', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095762,
     "end_time": "2023-12-12T12:45:26.808073",
     "exception": false,
     "start_time": "2023-12-12T12:45:26.712311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Fine_Tuning_YOLOv8\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤四 | 微调 YOLOv8 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0987,
     "end_time": "2023-12-12T12:45:27.003546",
     "exception": false,
     "start_time": "2023-12-12T12:45:26.904846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step of our project, we're set to fine-tune our YOLOv8 pre-trained object detection model using transfer learning, specifically tailoring it to our 'Top-View Vehicle Detection Image Dataset'. By leveraging the YOLOv8 model's existing weights from its training on the comprehensive COCO dataset, we start from a robust foundation rather than from scratch. This approach not only saves significant time and resources but also capitalizes on our focused dataset to enhance the model's ability to accurately recognize and detect vehicles in top-view images. This method of training enables efficient and effective model adaptation, ensuring it's finely attuned to the specificities of vehicle detection from aerial perspectives:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">在本项目的这个阶段，我们将使用迁移学习对YOLOv8预训练目标检测模型进行微调，使其专门适用于我们的“Top-View Vehicle Detection Image Dataset”。利用YOLOv8模型在COCO数据集上训练得到的现有权重，从一个稳健的基础开始，而不是从零开始。这种方法不仅节省了大量的时间和资源，而且还能利用我们专注的数据集来增强模型准确识别和检测空中俯视图像中车辆的能力。这种训练方法能够实现高效且有效的模型适应，确保模型能够精准地适应从空中视角检测车辆的特定需求。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:41:41.731615Z",
     "iopub.status.busy": "2024-01-07T16:41:41.731294Z",
     "iopub.status.idle": "2024-01-07T16:55:44.327808Z",
     "shell.execute_reply": "2024-01-07T16:55:44.32653Z",
     "shell.execute_reply.started": "2024-01-07T16:41:41.731585Z"
    },
    "papermill": {
     "duration": 23.627724,
     "end_time": "2023-12-12T12:45:50.728563",
     "exception": true,
     "start_time": "2023-12-12T12:45:27.100839",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model on our custom dataset\n",
    "results = model.train(\n",
    "    data=yaml_file_path,     # Path to the dataset configuration file\n",
    "    epochs=100,              # Number of epochs to train for\n",
    "    imgsz=640,               # Size of input images as integer\n",
    "    device=0,                # Device to run on, i.e. cuda device=0 \n",
    "    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n",
    "    batch=32,                # Number of images per batch\n",
    "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
    "    lr0=0.0001,              # Initial learning rate \n",
    "    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n",
    "    dropout=0.1,             # Use dropout regularization\n",
    "    seed=0                   # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🌐 Integration of Weights & Biases (Wandb) with YOLOv8</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Weights & Biases, known as <strong>wandb.ai</strong>, is an MLOps tool that works seamlessly with Ultralytics, including the YOLOv8 model. When we train our YOLOv8 model, <strong>wandb.ai</strong> helps to manage our machine learning experiments by monitoring the training process, logging important metrics, and saving outputs. It's like a dashboard where we can see how our model is learning, with all the details and visualizations to help us understand the training progress and results.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>🔑 Providing the API Key</b><br>\n",
    "        During the model training, an API key is required for Weights & Biases to track and store our model's data. We'll be prompted to sign up at <a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize</a> to get this key. After signing up, we copy the API key provided and paste it back into our training setup. This key connects our training session to the Weights & Biases platform, allowing us to access all the great features for monitoring and evaluating our model.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🌐 将 Weights & Biases (Wandb) 与 YOLOv8 集成</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Weights & Biases（简称<strong>wandb.ai</strong>）是一个与Ultralytics（包括YOLOv8模型）无缝协作的MLOps工具。在我们训练YOLOv8模型时，<strong>wandb.ai</strong>能够帮助管理我们的机器学习实验，通过监控训练过程、记录重要指标以及保存输出结果。它就像一个仪表盘，我们可以在这里看到模型的学习情况，所有细节和可视化信息都有助于我们理解训练进度和结果。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        <b>🔑 提供API密钥</b><br>\n",
    "        在模型训练期间，Weights & Biases需要一个API密钥来跟踪和存储我们模型的数据。我们需要在<a href=\"https://wandb.ai/authorize\">https://wandb.ai/authorize</a>注册以获取该密钥。注册后，我们将提供的API密钥复制并粘贴回我们的训练设置中。这个密钥将我们的训练会话连接到Weights & Biases平台，使我们能够使用所有用于监控和评估模型的出色功能。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Understanding Run Summary Metrics</b></h2>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Learning Rate per Group (lr/pg0, lr/pg1, lr/pg2):</b> These values represent the learning rate for different groups of layers in the neural network. A lower learning rate means the model updates its weights more slowly during training. Consistent learning rates across groups indicate uniform adjustments during the learning process.</li>\n",
    "        <li><b>Mean Average Precision at 50% IoU (metrics/mAP50(B)):</b> This metric measures the model's accuracy in detecting objects with at least 50% Intersection over Union (IoU) with ground truth. A score of 0.97 suggests the model is highly accurate at this IoU threshold.</li>\n",
    "        <li><b>Mean Average Precision across IoU from 50% to 95% (metrics/mAP50-95(B)):</b> This is an average of mAP calculated at different IoU thresholds, from 50% to 95%. A score of 0.74 indicates good overall accuracy across these varying thresholds.</li>\n",
    "        <li><b>Precision (metrics/precision(B)):</b> Precision measures the ratio of correctly predicted positive observations to the total predicted positives. A score of 0.92 means the model is highly precise in its predictions.</li>\n",
    "        <li><b>Recall (metrics/recall(B)):</b> Recall calculates the ratio of correctly predicted positive observations to all observations in actual class. A recall of 0.94 shows the model is very good at finding all relevant cases within the dataset.</li>\n",
    "        <li><b>Model Computational Complexity (model/GFLOPs):</b> Indicates the model's computational demands, with the GFLOPs value suggesting moderate complexity.</li>\n",
    "        <li><b>Model Parameters:</b> This is the total number of trainable parameters in the model. Almost 3 million parameters indicate a model of moderate size and complexity.</li>\n",
    "        <li><b>Inference Speed (model/speed_PyTorch(ms)):</b> The time taken for the model to make a single prediction (inference). 4.6 ms is quite fast, which is good for real-time applications.</li>\n",
    "        <li><b>Training Losses (train/box_loss, train/cls_loss, train/dfl_loss):</b>These are different types of losses during training. 'box_loss' refers to the error in bounding box predictions, 'cls_loss' to classification error, and 'dfl_loss' to distribution focal loss. Lower values indicate better performance.</li>\n",
    "        <li><b>Validation Losses (val/box_loss, val/cls_loss, val/dfl_loss):</b> Similar to training losses, these are losses calculated on the validation dataset. They give an idea of how well the model generalizes to new, unseen data. Almost similar loss values for both training and validation indicate that the model is not overfitting.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 理解运行结束后的总结指标</b></h2>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>每组的学习率（lr/pg0、lr/pg1、lr/pg2）：</b>这些值表示神经网络中不同层组的学习率。较低的学习率意味着模型在训练过程中更缓慢地更新其权重。各组之间一致的学习率表明在学习过程中进行了均匀的调整。</li>\n",
    "        <li><b>50% IoU 下的平均精度均值（metrics/mAP50(B)）：</b>该指标衡量模型在与真实边界框至少有 50% 交并比（IoU）时检测目标的准确性。0.97 的分数表明模型在此 IoU 阈值下具有很高的准确性。</li>\n",
    "        <li><b>50% 到 95% IoU 范围内的平均精度均值（metrics/mAP50-95(B)）：</b>这是在不同 IoU 阈值（从 50% 到 95%）下计算的 mAP 的平均值。0.74 的分数表明模型在这些不同阈值下具有良好的整体准确性。</li>\n",
    "        <li><b>精确率（metrics/precision(B)）：</b>精确率衡量正确预测为正的观测值与总预测为正的观测值的比率。0.92 的分数表明模型在其预测中具有很高的精确性。</li>\n",
    "        <li><b>召回率（metrics/recall(B)）：</b>召回率计算正确预测为正的观测值与实际类别中所有观测值的比率。0.94 的召回率表明模型在数据集中找到所有相关案例方面表现出色。</li>\n",
    "        <li><b>模型计算复杂度（model/GFLOPs）：</b>表明模型的计算需求，GFLOPs 值表明复杂度适中。</li>\n",
    "        <li><b>模型参数：</b>这是模型中可训练参数的总数。接近 300 万的参数表明模型的大小和复杂度适中。</li>\n",
    "        <li><b>推理速度（model/speed_PyTorch(ms)）：</b>模型进行单次预测（推理）所需的时间。4.6 毫秒的速度相当快，适合实时应用。</li>\n",
    "        <li><b>训练损失（train/box_loss、train/cls_loss、train/dfl_loss）：</b>这些是在训练过程中不同类型的损失。“box_loss”指的是边界框预测的误差，“cls_loss”指的是分类误差，“dfl_loss”指的是分布焦点损失。较低的值表示性能更好。</li>\n",
    "        <li><b>验证损失（val/box_loss、val/cls_loss、val/dfl_loss）：</b>与训练损失类似，这些是在验证数据集上计算的损失。它们可以让我们了解模型对新的、未见过的数据的泛化能力。训练和验证的损失值几乎相同表明模型没有过拟合。</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Performance_Evaluation\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤五 | 模型性能评估 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Post-training, our model generates several files and folders that encapsulate various aspects of the training run. Let's see the list of generated files:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">训练完成后，模型会生成多个文件和文件夹，这些文件和文件夹涵盖了训练运行的各个方面。现在看看生成的文件列表：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:44.33068Z",
     "iopub.status.busy": "2024-01-07T16:55:44.330087Z",
     "iopub.status.idle": "2024-01-07T16:55:45.379173Z",
     "shell.execute_reply": "2024-01-07T16:55:45.377605Z",
     "shell.execute_reply.started": "2024-01-07T16:55:44.330643Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory\n",
    "post_training_files_path = '/kaggle/working/runs/detect/train'\n",
    "\n",
    "# List the files in the directory\n",
    "!ls {post_training_files_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📁 Training Output Files Explained</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Here’s a rundown of each item:\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Weights Folder:</b> Contains the 'best.pt' and 'last.pt' files, which are the best and most recent weights of our trained model respectively.</li>\n",
    "        <li><b>Args:</b> A file that stores the arguments or parameters that were used during the training process.</li>\n",
    "        <li><b>Confusion Matrix:</b> Visual representations of the model performance. One is normalized, which helps in understanding the true positive rate across classes.</li>\n",
    "        <li><b>Events File:</b> Contains logs of events that occurred during training, useful for debugging and analysis.</li>\n",
    "        <li><b>F1 Curve:</b> Illustrates the F1 score of the model over time, balancing precision and recall.</li>\n",
    "        <li><b>Labels:</b> Shows the distribution of different classes within the dataset and their correlation.</li>\n",
    "        <li><b>P Curve, PR Curve, R Curve:</b> These are Precision, Precision-Recall, and Recall curves, respectively, providing insights into the trade-offs between different metrics at various thresholds.</li>\n",
    "        <li><b>results:</b> This csv file captures a comprehensive set of performance metrics recorded at each epoch during the model's training process.</li>\n",
    "        <li><b>Train Batch Images:</b> Sample images from the training set with model predictions overlaid, useful for a quick visual check of model performance.</li>\n",
    "        <li><b>Validation Batch Images:</b> Similar to train batch images, these are from the validation set and include both labels and predictions, providing a glimpse into how well the model generalizes.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📁 训练输出文件说明</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        以下是每个文件的说明：\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>权重文件夹：</b>包含“best.pt”和“last.pt”文件，分别是训练模型的最佳权重和最新权重。</li>\n",
    "        <li><b>参数文件：</b>存储训练过程中使用的参数或参数设置的文件。</li>\n",
    "        <li><b>混淆矩阵：</b>模型性能的可视化表示。其中一个是归一化的，有助于理解不同类别的真正阳性比率。</li>\n",
    "        <li><b>事件文件：</b>记录训练过程中发生的事件日志，对调试和分析非常有用。</li>\n",
    "        <li><b>F1 曲线：</b>展示模型随时间变化的 F1 分数，平衡精确率和召回率。</li>\n",
    "        <li><b>标签分布：</b>展示数据集中不同类别的分布及其相关性。</li>\n",
    "        <li><b>P 曲线、PR 曲线、R 曲线：</b>分别是精确率曲线、精确率-召回率曲线和召回率曲线，提供了在不同阈值下不同指标之间的权衡。</li>\n",
    "        <li><b>结果文件：</b>这个 csv 文件记录了模型训练过程中每个epoch的全面性能指标。</li>\n",
    "        <li><b>训练批次图像：</b>训练集中的样本图像，叠加了模型预测结果，可用于快速直观检查模型性能。</li>\n",
    "        <li><b>验证批次图像：</b>与训练批次图像类似，这些来自验证集的图像包括标签和预测结果，提供了模型泛化能力的直观展示。</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        I will undertake a comprehensive evaluation and analysis of our model's performance, which involves:\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Learning Curves Analysis</b></li>\n",
    "        <li><b>Confusion Matrix Evaluation</b></li>\n",
    "        <li><b>Performance Metrics Assessment</b></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        下面将对模型的性能进行全面评估和分析，包括：\n",
    "    </p>\n",
    "    <ul style=\"font-size:18px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>学习曲线分析</b></li>\n",
    "        <li><b>混淆矩阵评估</b></li>\n",
    "        <li><b>性能指标评估</b></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Learning_Curves\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 5.1 |</span><span style='color:#141140'> 学习曲线分析</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">In this step, let's review the training and validation loss trends over epochs to assess the learning stability and efficiency:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">在这一步中，我们回顾一下训练和验证损失随周期的变化趋势，以评估学习的稳定性和效率：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.381771Z",
     "iopub.status.busy": "2024-01-07T16:55:45.381361Z",
     "iopub.status.idle": "2024-01-07T16:55:45.391793Z",
     "shell.execute_reply": "2024-01-07T16:55:45.390676Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.381734Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a function to plot learning curves for loss values\n",
    "def plot_learning_curve(df, train_loss_col, val_loss_col, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df, x='epoch', y=train_loss_col, label='Train Loss', color='#141140', linestyle='-', linewidth=2)\n",
    "    sns.lineplot(data=df, x='epoch', y=val_loss_col, label='Validation Loss', color='orangered', linestyle='--', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:45.393767Z",
     "iopub.status.busy": "2024-01-07T16:55:45.393389Z",
     "iopub.status.idle": "2024-01-07T16:55:46.863097Z",
     "shell.execute_reply": "2024-01-07T16:55:46.862056Z",
     "shell.execute_reply.started": "2024-01-07T16:55:45.393731Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the full file path for 'results.csv' using the directory path and file name\n",
    "results_csv_path = os.path.join(post_training_files_path, 'results.csv')\n",
    "\n",
    "# Load the CSV file from the constructed path into a pandas DataFrame\n",
    "df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Remove any leading whitespace from the column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Plot the learning curves for each loss\n",
    "plot_learning_curve(df, 'train/box_loss', 'val/box_loss', 'Box Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/cls_loss', 'val/cls_loss', 'Classification Loss Learning Curve')\n",
    "plot_learning_curve(df, 'train/dfl_loss', 'val/dfl_loss', 'Distribution Focal Loss Learning Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 Model Learning Curve Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The learning curves for box loss, classification loss, and distribution focal loss indicate a rapid decrease in loss values during the initial epochs, leveling off as training progresses. This trend, along with the close alignment of training and validation loss lines, suggests that the model is learning effectively without overfitting, meaning it is well-tuned to the dataset without being biased or too variable.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The smoothness of the learning curves, especially evident in the latter epochs, implies that the model is reaching a state of equilibrium, where additional training does not significantly enhance performance. This observation suggests that 100 epochs are sufficient for training this model, as further training is unlikely to result in substantial gains.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>📈 模型学习曲线分析</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        边界框损失、分类损失和分布焦点损失的学习曲线表明，在初始训练周期内损失值迅速下降，随着训练的进行逐渐趋于平稳。这种趋势，加上训练损失和验证损失线的紧密对齐，表明模型在学习过程中表现良好，没有出现过拟合现象，即模型能够很好地适应数据集，既不过于偏向某个方向，也不过于多变。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        学习曲线在后期表现出的平滑性表明模型已达到一种平衡状态，额外的训练不会显著提升性能。这一观察结果表明，100个训练周期对于训练该模型是足够的，因为进一步的训练不太可能带来显著的性能提升。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Confusion_Matrix\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 5.2 |</span><span style='color:#141140'> 混淆矩阵评估</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Next, I will focus on displaying and meticulously analyzing the confusion matrix derived from our model's performance on the validation dataset:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        接下来，将专注于展示、并仔细分析模型在验证数据集上得出的混淆矩阵：\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:46.865642Z",
     "iopub.status.busy": "2024-01-07T16:55:46.864744Z",
     "iopub.status.idle": "2024-01-07T16:55:48.072583Z",
     "shell.execute_reply": "2024-01-07T16:55:48.070774Z",
     "shell.execute_reply.started": "2024-01-07T16:55:46.865601Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Construct the path to the normalized confusion matrix image\n",
    "confusion_matrix_path = os.path.join(post_training_files_path, 'confusion_matrix_normalized.png')\n",
    "\n",
    "# Read the image using cv2\n",
    "cm_img = cv2.imread(confusion_matrix_path)\n",
    "\n",
    "# Convert the image from BGR to RGB color space for accurate color representation with matplotlib\n",
    "cm_img = cv2.cvtColor(cm_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10), dpi=120)\n",
    "plt.imshow(cm_img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 Confusion Matrix Analysis</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The confusion matrix for our YOLOv8 vehicle detection model illustrates high accuracy as mentioned earlier as well. In 97% of instances, the model successfully identifies the presence of a vehicle when there is one, indicating strong detection capability. Conversely, in just 3% of cases, the model fails to detect a vehicle that is actually present, suggesting room for improvement in reducing false negatives.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🔍 混淆矩阵分析</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        YOLOv8车辆检测模型的混淆矩阵显示了较高的准确性。在97%的情况下，模型能够成功检测到实际存在的车辆，表明其具有强大的检测能力。相反，在仅3%的情况下，模型未能检测到实际存在的车辆，这表明在减少假阴性方面还有改进的空间。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Performance_Metrics\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 5.3 |</span><span style='color:#141140'> 性能指标评估</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">Finally, I am delving into various metrics to understand the model's predictive accuracy and areas of potential improvement:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">最后，将深入研究各种指标，以了解模型的预测准确性以及潜在的改进方向：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:55:48.078246Z",
     "iopub.status.busy": "2024-01-07T16:55:48.077923Z",
     "iopub.status.idle": "2024-01-07T16:56:00.887425Z",
     "shell.execute_reply": "2024-01-07T16:56:00.886243Z",
     "shell.execute_reply.started": "2024-01-07T16:55:48.07822Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Construct the path to the best model weights file using os.path.join\n",
    "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
    "\n",
    "# Load the best model weights into the YOLO model\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Validate the best model using the validation set with default parameters\n",
    "metrics = best_model.val(split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "as can be seen in the above verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.890082Z",
     "iopub.status.busy": "2024-01-07T16:56:00.88966Z",
     "iopub.status.idle": "2024-01-07T16:56:00.909676Z",
     "shell.execute_reply": "2024-01-07T16:56:00.908441Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.890045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas DataFrame and use the keys as the index\n",
    "metrics_df = pd.DataFrame.from_dict(metrics.results_dict, orient='index', columns=['Metric Value'])\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 Model Evaluation Insights</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        The YOLOv8 model shows impressive results on the validation set. With a precision of <b>91.6%</b>, it indicates that the majority of the predictions made by the model are correct. The recall score of <b>93.8%</b> demonstrates the model's ability to find most of the relevant cases in the dataset. The model's mean Average Precision (mAP) at 50% Intersection over Union (IoU) is <b>97.5%</b>, reflecting high accuracy in detecting objects with a considerable overlap with the ground truth. Even when the IoU threshold range is expanded from 50% to 95%, the model maintains a solid mAP of <b>74.2%</b>. Finally, the fitness score of <b>76.5%</b> indicates a good balance between precision, recall, and the IoU of the predictions, confirming the model's effectiveness in object detection tasks.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "    <h1 style=\"font-size:24px; font-family:calibri; color:#141140;\"><b>🔍 模型评估洞察</b></h1>\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        YOLOv8模型在验证集上表现出色，其精确率为<b>91.6%</b>，表明模型所做的大多数目标检测任务都是正确的；召回率为<b>93.8%</b>，显示了模型在数据集中具备检测大多数相关目标类型的能力。模型在50%交并比（IoU）下的平均精度均值（mAP）为<b>97.5%</b>，表明在与真实边界框有较大重叠的情况下，检测目标的准确性很高。即使将IoU阈值范围从50%扩展到95%，模型仍保持了<b>74.2%</b>的稳健mAP。最后，<b>76.5%</b>的适应度评分表明在精确率、召回率以及预测的IoU之间取得了良好的平衡，确认了模型在目标检测任务中的有效性[^38^]。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Inference\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤六 | 模型推理与泛化能力评估 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To thoroughly assess our model's capability to generalize, I will conduct inferences in three distinct steps:\n",
    "    </p>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>Inference on Validation Set Images</b></li>\n",
    "        <li><b>Inference on an Unseen Test Image</b></li>\n",
    "        <li><b>Inference on an Unseen Test Video</b></li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        为了全面评估模型的泛化能力，下面将分三步进行推理展示：\n",
    "    </p>\n",
    "    <ul style=\"font-size:20px; font-family:calibri; line-height: 1.5em;\">\n",
    "        <li><b>验证集图像推理</b></li>\n",
    "        <li><b>未见测试图像推理</b></li>\n",
    "        <li><b>未见测试视频推理</b></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Validation\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 6.1 |</span><span style='color:#141140'> 验证集图像推理</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\"> First of all, I am going to evaluate model predictions on random images from the validation dataset:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">首先，我们在验证数据集中随机选取图像，然后评估模型的预测结果：</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:00.911548Z",
     "iopub.status.busy": "2024-01-07T16:56:00.911155Z",
     "iopub.status.idle": "2024-01-07T16:56:04.95926Z",
     "shell.execute_reply": "2024-01-07T16:56:04.957846Z",
     "shell.execute_reply.started": "2024-01-07T16:56:00.911511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the validation images\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 9 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
    "\n",
    "# Initialize the subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
    "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
    "\n",
    "# Perform inference on each selected image and display it\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
    "    results = best_model.predict(source=image_path, imgsz=640, conf=0.5)\n",
    "    annotated_image = results[0].plot(line_width=1)\n",
    "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(annotated_image_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Test_Image\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 6.2 |</span><span style='color:#141140'> 未见测试图像推理</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Now, I will employ the best version of our fine-tuned model to evaluate its generalization capabilities. I'll test it on the same image previously analyzed using the pre-trained YOLOv8 model on the COCO dataset:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        接下来，使用我们微调后的最佳模型版本来评估其泛化能力。下面，将在之前使用预训练YOLOv8模型在COCO数据集上分析过的同一张图像上进行测试：\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:04.961504Z",
     "iopub.status.busy": "2024-01-07T16:56:04.96104Z",
     "iopub.status.idle": "2024-01-07T16:56:06.324053Z",
     "shell.execute_reply": "2024-01-07T16:56:06.322966Z",
     "shell.execute_reply.started": "2024-01-07T16:56:04.961454Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the image file\n",
    "sample_image_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'\n",
    "\n",
    "# Perform inference on the provided image using best model\n",
    "results = best_model.predict(source=sample_image_path, imgsz=640, conf=0.7) \n",
    "                        \n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv8 Model', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 Enhanced Vehicle Detection with Fine-Tuning</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        The comparison between the above image with the image we had in step 2 clearly demonstrates the benefits of fine-tuning the YOLOv8 model for a specialized task. In the image from step 2, the pre-trained model on the COCO dataset missed detecting a truck and misclassified it, indicating limitations when dealing with a specific class of objects due to its broader training scope. In contrast, the above image shows that the fine-tuned model on a vehicle-specific dataset has accurately detected and classified various vehicles, including the previously missed truck. This improvement highlights the model's enhanced capability to discern features specific to vehicles, leading to better precision and recall in vehicle detection.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <h2 style=\"font-size:22px; font-family:calibri; color:#141140;\"><b>🚗 通过微调提升车辆检测性能</b></h2>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        将上述图像与第2步中的图像进行对比，清晰地展示了为特定任务微调YOLOv8模型的好处。在第2步的图像中，基于COCO数据集预训练的模型，未能检测到一辆卡车并将其错误分类，这表明由于其训练类别过于宽泛，在处理特定类别物体时会存在局限性。相比之下，在专门针对车辆数据集上微调后，上述图像显示了该模型能够准确检测并分类各种车辆，包括之前遗漏的卡车。这一改进突出了模型在识别车辆特定特征方面增强的能力，从而在车辆检测中实现了更高的精确率和召回率。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Inference_Test_Video\"></a>\n",
    "# <b><span style='color:#b2addb'>步骤 6.3 |</span><span style='color:#141140'> 未见测试视频推理</span></b>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Finally, I am going to assess the model's generalization capabilities on a completely new video, unseen during training. This step is crucial to demonstrate the model's ability to adapt and perform accurately in real-world applications, further solidifying its effectiveness outside of the controlled dataset environment:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        最后，将在一个全新的视频上评估模型的泛化能力，这个视频在训练过程中从未见过。这一步对于展示模型在现实世界应用中适应和准确执行能力至关重要，进一步证明了模型在受控数据集环境之外的有效性。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:06.325982Z",
     "iopub.status.busy": "2024-01-07T16:56:06.325408Z",
     "iopub.status.idle": "2024-01-07T16:56:25.874908Z",
     "shell.execute_reply": "2024-01-07T16:56:25.873977Z",
     "shell.execute_reply.started": "2024-01-07T16:56:06.32595Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the path to the sample video in the dataset\n",
    "dataset_video_path = '/kaggle/input/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'\n",
    "\n",
    "# Define the destination path in the working directory\n",
    "video_path = '/kaggle/working/sample_video.mp4'\n",
    "\n",
    "# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\n",
    "shutil.copyfile(dataset_video_path, video_path)\n",
    "\n",
    "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
    "best_model.predict(source=video_path, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        To ensure compatibility with various platforms, including Jupyter Notebooks, we'll convert the output <code>.avi</code> video file to the more universally supported <code>.mp4</code> format and then display it within the notebook environment:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        为了确保与各种平台（包括Jupyter Notebook）的兼容性，下面将输出的<code>.avi</code>视频文件转换为更通用的<code>.mp4</code>格式，然后在笔记本环境中显示它：\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:25.876607Z",
     "iopub.status.busy": "2024-01-07T16:56:25.876267Z",
     "iopub.status.idle": "2024-01-07T16:56:40.862642Z",
     "shell.execute_reply": "2024-01-07T16:56:40.860731Z",
     "shell.execute_reply.started": "2024-01-07T16:56:25.876578Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /kaggle/working/runs/detect/predict/sample_video.avi processed_sample_video.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"processed_sample_video.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Traffic_Intensity_Estimation\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤七 | 实时交通流量估计 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        As we transition into the practical application step of our project, we are set to deploy our finely-tuned vehicle detection model to analyze traffic density. This step is crucial in demonstrating the model's ability to generalize and perform accurately on unseen videos—videos that were not part of the model's training or validation sets.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "Our objective is to quantify the traffic by counting vehicles within specified areas on the road lanes, frame by frame. The analysis will not only reveal the vehicle count but also gauge the intensity of traffic, labeling it as 'Heavy' or 'Smooth' based on a predetermined threshold. The count and traffic flow insights are pivotal for urban planning and traffic management.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "While real-time processing on Kaggle is not viable, the code below simulates this process by modifying video frames, applying vehicle detection, and annotating the results. This mimics real-time analysis which can be achieved on local machines—even on CPUs—by processing webcam feeds or video files in real-time. The annotated video is then saved, ready to be reviewed for traffic assessment.\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "Let's delve into the code that brings this all to life:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        现在进入项目实际应用阶段，下面将部署我们精心微调的车辆检测模型来分析交通流量。这一步对于展示模型在未见过的视频（即不在模型训练或验证集中的视频）上泛化和准确执行的能力至关重要。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        我们的目标是通过在道路车道的指定区域内逐帧统计车辆数量来量化交通流量。分析不仅会揭示车辆数量，还会根据预设阈值评估交通强度，将其标记为“拥堵”或“顺畅”。车辆计数和交通流量的见解对于城市规划和交通管理至关重要。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        尽管在Kaggle上无法实现实时处理，但下面的代码通过修改视频帧、应用车辆检测并标注结果来模拟这一过程。这可以在本地机器上实现，甚至在CPU上，通过实时处理网络摄像头输入或视频文件来实现。标注后的视频将被保存，以便进行交通评估。\n",
    "    </p>\n",
    "    <p style=\"font-size:18px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        下面深入了解实现上述这一切的代码：\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:40.865451Z",
     "iopub.status.busy": "2024-01-07T16:56:40.865105Z",
     "iopub.status.idle": "2024-01-07T16:56:55.842674Z",
     "shell.execute_reply": "2024-01-07T16:56:55.841793Z",
     "shell.execute_reply.started": "2024-01-07T16:56:40.865421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the threshold for considering traffic as heavy\n",
    "heavy_traffic_threshold = 10\n",
    "\n",
    "# Define the vertices for the quadrilaterals\n",
    "vertices1 = np.array([(465, 350), (609, 350), (510, 630), (2, 630)], dtype=np.int32)\n",
    "vertices2 = np.array([(678, 350), (815, 350), (1203, 630), (743, 630)], dtype=np.int32)\n",
    "\n",
    "# Define the vertical range for the slice and lane threshold\n",
    "x1, x2 = 325, 635 \n",
    "lane_threshold = 609\n",
    "\n",
    "# Define the positions for the text annotations on the image\n",
    "text_position_left_lane = (10, 50)\n",
    "text_position_right_lane = (820, 50)\n",
    "intensity_position_left_lane = (10, 100)\n",
    "intensity_position_right_lane = (820, 100)\n",
    "\n",
    "# Define font, scale, and colors for the annotations\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (255, 255, 255)    # White color for text\n",
    "background_color = (0, 0, 255)  # Red background for text\n",
    "        \n",
    "# Open the video\n",
    "cap = cv2.VideoCapture('sample_video.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('traffic_density_analysis.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Read until video is completed\n",
    "while cap.isOpened():\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Create a copy of the original frame to modify\n",
    "        detection_frame = frame.copy()\n",
    "    \n",
    "        # Black out the regions outside the specified vertical range\n",
    "        detection_frame[:x1, :] = 0  # Black out from top to x1\n",
    "        detection_frame[x2:, :] = 0  # Black out from x2 to the bottom of the frame\n",
    "        \n",
    "        # Perform inference on the modified frame\n",
    "        results = best_model.predict(detection_frame, imgsz=640, conf=0.4)\n",
    "        processed_frame = results[0].plot(line_width=1)\n",
    "        \n",
    "        # Restore the original top and bottom parts of the frame\n",
    "        processed_frame[:x1, :] = frame[:x1, :].copy()\n",
    "        processed_frame[x2:, :] = frame[x2:, :].copy()        \n",
    "        \n",
    "        # Draw the quadrilaterals on the processed frame\n",
    "        cv2.polylines(processed_frame, [vertices1], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        cv2.polylines(processed_frame, [vertices2], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        # Retrieve the bounding boxes from the results\n",
    "        bounding_boxes = results[0].boxes\n",
    "\n",
    "        # Initialize counters for vehicles in each lane\n",
    "        vehicles_in_left_lane = 0\n",
    "        vehicles_in_right_lane = 0\n",
    "\n",
    "        # Loop through each bounding box to count vehicles in each lane\n",
    "        for box in bounding_boxes.xyxy:\n",
    "            # Check if the vehicle is in the left lane based on the x-coordinate of the bounding box\n",
    "            if box[0] < lane_threshold:\n",
    "                vehicles_in_left_lane += 1\n",
    "            else:\n",
    "                vehicles_in_right_lane += 1\n",
    "                \n",
    "        # Determine the traffic intensity for the left lane\n",
    "        traffic_intensity_left = \"Heavy\" if vehicles_in_left_lane > heavy_traffic_threshold else \"Smooth\"\n",
    "        # Determine the traffic intensity for the right lane\n",
    "        traffic_intensity_right = \"Heavy\" if vehicles_in_right_lane > heavy_traffic_threshold else \"Smooth\"\n",
    "\n",
    "\n",
    "        # Add a background rectangle for the left lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_left_lane[0]-10, text_position_left_lane[1] - 25), \n",
    "                      (text_position_left_lane[0] + 460, text_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Left Lane: {vehicles_in_left_lane}', text_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the left lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_left_lane[0]-10, intensity_position_left_lane[1] - 25), \n",
    "                      (intensity_position_left_lane[0] + 460, intensity_position_left_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the left lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_left}', intensity_position_left_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane vehicle count\n",
    "        cv2.rectangle(processed_frame, (text_position_right_lane[0]-10, text_position_right_lane[1] - 25), \n",
    "                      (text_position_right_lane[0] + 460, text_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the vehicle count text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Vehicles in Right Lane: {vehicles_in_right_lane}', text_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Add a background rectangle for the right lane traffic intensity\n",
    "        cv2.rectangle(processed_frame, (intensity_position_right_lane[0]-10, intensity_position_right_lane[1] - 25), \n",
    "                      (intensity_position_right_lane[0] + 460, intensity_position_right_lane[1] + 10), background_color, -1)\n",
    "\n",
    "        # Add the traffic intensity text on top of the rectangle for the right lane\n",
    "        cv2.putText(processed_frame, f'Traffic Intensity: {traffic_intensity_right}', intensity_position_right_lane, \n",
    "                    font, font_scale, font_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "        # Uncomment the following 3 lines if running this code on a local machine to view the real-time processing results\n",
    "        # cv2.imshow('Real-time Analysis', processed_frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press Q on keyboard to exit the loop\n",
    "        #     break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all the frames\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        Finally, lets convert the output <code>.avi</code> video to <code>.mp4</code> format for notebook playback and display it:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        最后，将输出的<code>.avi</code>视频转换为<code>.mp4</code>格式，以便在笔记本中播放和显示：\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:56:55.844457Z",
     "iopub.status.busy": "2024-01-07T16:56:55.844123Z",
     "iopub.status.idle": "2024-01-07T16:57:08.599235Z",
     "shell.execute_reply": "2024-01-07T16:57:08.597866Z",
     "shell.execute_reply.started": "2024-01-07T16:56:55.844428Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by our traffic density estimation app to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /kaggle/working/traffic_density_analysis.avi traffic_density_analysis.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"traffic_density_analysis.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a id=\"Model_Export\"></a>\n",
    "# <p style=\"background-color: #141140; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">步骤 八 | 模型导出以实现跨平台部署 </p>\n",
    "⬆️ [Tabel of Contents](#contents_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        We're now at the final step of our project – exporting our best model for future use and deployment. Currently, our model exists in a <code>.pt</code> format, optimal for tasks such as ongoing training, additional fine-tuning, or deployment within PyTorch-compatible environments. To enhance the model's versatility and ensure it can be utilized across various platforms, we'll be exporting it in the ONNX (Open Neural Network Exchange) format. ONNX is specially designed for model portability, enabling seamless operation across different machine learning frameworks including PyTorch, TensorFlow, and Microsoft's Cognitive Toolkit (CNTK), as well as compatibility with specialized hardware accelerators. This step ensures our model's readiness for diverse deployment scenarios, aligning with the broad scope of modern AI applications. For more details on the export process and available formats, refer to <a href=\"https://docs.ultralytics.com/modes/export/#arguments\">Ultralytics Documentation</a>.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#eae8fa; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1);\">\n",
    "    <p style=\"font-size:20px; font-family:calibri; line-height: 1.5em; text-indent: 20px;\">\n",
    "        现在到了项目最后一步——导出最佳模型以便未来使用和部署。目前，我们的模型以<code>.pt</code>格式存在，这种格式非常适合持续训练、进一步微调或在支持PyTorch环境中部署。为了增强模型的通用性并确保它可以在各种平台上使用，我们将把它导出为ONNX（Open Neural Network Exchange）格式。ONNX是专门为模型移植性设计的，能够在包括PyTorch、TensorFlow以及微软认知工具包（CNTK）在内的不同机器学习框架之间无缝运行，同时也兼容专用硬件加速器。这一步确保了该模型能够适应各种部署场景，符合现代人工智能应用的广泛需求。关于导出过程和可用格式的更多详细信息，请参考<a href=\"https://docs.ultralytics.com/modes/export/#arguments\">Ultralytics文档</a>。\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T16:57:08.601218Z",
     "iopub.status.busy": "2024-01-07T16:57:08.600853Z",
     "iopub.status.idle": "2024-01-07T16:57:10.881191Z",
     "shell.execute_reply": "2024-01-07T16:57:10.880392Z",
     "shell.execute_reply.started": "2024-01-07T16:57:08.601188Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the model\n",
    "best_model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color: #eae8fa; font-size: 120%; text-align: center;\">\n",
    "    <strong>\n",
    "        🌐 For comprehensive insights, extensive code, and additional resources, visit the project's \n",
    "        <a href=\"https://github.com/FarzadNekouee/YOLOv8_Traffic_Density_Estimation/tree/master\" style=\"color: #141140; text-decoration: none;\">\n",
    "            <em><u>GitHub Repository</u></em>\n",
    "        </a> \n",
    "        🌐\n",
    "    </strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<h2 align=\"left\"><font color=#141140>Best Regards</font></h2>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4107330,
     "sourceId": 7166601,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 68.691048,
   "end_time": "2023-12-12T12:45:53.82776",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T12:44:45.136712",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
